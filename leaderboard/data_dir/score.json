{
  "Athene-70B": {
    "model": "Athene-70B",
    "score": 7.970645792563601,
    "adjusted_score": 5.9412915851272015,
    "task_macro_score": 5.953736733195851,
    "adjusted_task_macro_score": 5.953736733195851,
    "task_categorized_scores": {
      "Coding & Debugging": 5.895734597156398,
      "Creative Tasks": 6.036175710594314,
      "Planning & Reasoning": 6.095952023988005,
      "Information/Advice seeking": 6.079207920792079,
      "Math & Data Analysis": 5.713147410358566
    },
    "total": 1022,
    "avg_len": 3175.1438356164385
  },
  "gpt-4o-2024-05-13": {
    "model": "gpt-4o-2024-05-13",
    "score": 7.940371456500489,
    "adjusted_score": 5.880742913000978,
    "task_macro_score": 5.929817880351956,
    "adjusted_task_macro_score": 5.929817880351956,
    "task_categorized_scores": {
      "Coding & Debugging": 6.0473933649289116,
      "Creative Tasks": 5.912144702842378,
      "Planning & Reasoning": 6.020958083832337,
      "Information/Advice seeking": 5.861386138613861,
      "Math & Data Analysis": 5.729083665338646
    },
    "total": 1023,
    "avg_len": 3723.516129032258
  },
  "gpt-4o-mini-2024-07-18": {
    "model": "gpt-4o-mini-2024-07-18",
    "score": 7.86328125,
    "adjusted_score": 5.7265625,
    "task_macro_score": 5.713689403451416,
    "adjusted_task_macro_score": 5.713689403451416,
    "task_categorized_scores": {
      "Coding & Debugging": 5.716981132075471,
      "Creative Tasks": 6.00516795865633,
      "Planning & Reasoning": 5.823617339312406,
      "Information/Advice seeking": 5.742574257425742,
      "Math & Data Analysis": 5.404761904761905
    },
    "total": 1024,
    "avg_len": 3648.126953125
  },
  "gpt-4-turbo-2024-04-09": {
    "model": "gpt-4-turbo-2024-04-09",
    "score": 7.804496578690127,
    "adjusted_score": 5.6089931573802545,
    "task_macro_score": 5.522122481039269,
    "adjusted_task_macro_score": 5.522122481039269,
    "task_categorized_scores": {
      "Coding & Debugging": 5.507109004739336,
      "Creative Tasks": 5.865633074935401,
      "Planning & Reasoning": 5.6203288490284,
      "Information/Advice seeking": 5.717821782178218,
      "Math & Data Analysis": 5.099601593625499
    },
    "total": 1023,
    "avg_len": 3093.1700879765394
  },
  "Mistral-Large-2": {
    "model": "Mistral-Large-2",
    "score": 7.7900390625,
    "adjusted_score": 5.580078125,
    "task_macro_score": 5.556833516154802,
    "adjusted_task_macro_score": 5.556833516154802,
    "task_categorized_scores": {
      "Planning & Reasoning": 5.721556886227544,
      "Information/Advice seeking": 5.737623762376238,
      "Coding & Debugging": 5.383886255924171,
      "Creative Tasks": 5.8860103626943,
      "Math & Data Analysis": 5.266932270916335
    },
    "total": 1024,
    "avg_len": 3503.6262230919765
  },
  "yi-large-preview": {
    "model": "yi-large-preview",
    "score": 7.741935483870968,
    "adjusted_score": 5.483870967741936,
    "task_macro_score": 5.529462523202478,
    "adjusted_task_macro_score": 5.529462523202478,
    "task_categorized_scores": {
      "Planning & Reasoning": 5.66066066066066,
      "Information/Advice seeking": 5.772277227722773,
      "Coding & Debugging": 5.428571428571429,
      "Creative Tasks": 5.7643979057591626,
      "Math & Data Analysis": 5.192
    },
    "total": 1023,
    "avg_len": 3512.678149606299
  },
  "claude-3-5-sonnet-20240620": {
    "model": "claude-3-5-sonnet-20240620",
    "score": 7.7265625,
    "adjusted_score": 5.453125,
    "task_macro_score": 5.469508456618439,
    "adjusted_task_macro_score": 5.469508456618439,
    "task_categorized_scores": {
      "Coding & Debugging": 5.650943396226415,
      "Creative Tasks": 5.560723514211887,
      "Planning & Reasoning": 5.563527653213752,
      "Information/Advice seeking": 5.554455445544555,
      "Math & Data Analysis": 5.015873015873016
    },
    "total": 1024,
    "avg_len": 2911.845703125
  },
  "gemma-2-9b-it-DPO": {
    "model": "gemma-2-9b-it-DPO",
    "score": 7.712890625,
    "adjusted_score": 5.42578125,
    "task_macro_score": 5.322295446230848,
    "adjusted_task_macro_score": 5.322295446230848,
    "task_categorized_scores": {
      "Planning & Reasoning": 5.547226386806596,
      "Information/Advice seeking": 5.821782178217822,
      "Coding & Debugging": 5.052132701421801,
      "Creative Tasks": 5.9067357512953365,
      "Math & Data Analysis": 4.712
    },
    "total": 1024,
    "avg_len": 3982.628795298727
  },
  "gemma-2-9b-it-SimPO": {
    "model": "gemma-2-9b-it-SimPO",
    "score": 7.703812316715543,
    "adjusted_score": 5.407624633431086,
    "task_macro_score": 5.327923406955029,
    "adjusted_task_macro_score": 5.327923406955029,
    "task_categorized_scores": {
      "Planning & Reasoning": 5.564564564564565,
      "Information/Advice seeking": 5.648514851485148,
      "Coding & Debugging": 5.085714285714285,
      "Creative Tasks": 5.797927461139896,
      "Math & Data Analysis": 4.859437751004016
    },
    "total": 1023,
    "avg_len": 4277.667647058824
  },
  "deepseek-v2-chat-0628": {
    "model": "deepseek-v2-chat-0628",
    "score": 7.6904296875,
    "adjusted_score": 5.380859375,
    "task_macro_score": 5.399428041165569,
    "adjusted_task_macro_score": 5.399428041165569,
    "task_categorized_scores": {
      "Coding & Debugging": 5.5,
      "Creative Tasks": 5.643410852713178,
      "Planning & Reasoning": 5.482810164424514,
      "Information/Advice seeking": 5.272277227722773,
      "Math & Data Analysis": 5.142857142857142
    },
    "total": 1024,
    "avg_len": 3252.376953125
  },
  "gpt-4-0125-preview": {
    "model": "gpt-4-0125-preview",
    "score": 7.6640625,
    "adjusted_score": 5.328125,
    "task_macro_score": 5.227753918256898,
    "adjusted_task_macro_score": 5.227753918256898,
    "task_categorized_scores": {
      "Coding & Debugging": 5.2924528301886795,
      "Creative Tasks": 5.757105943152455,
      "Planning & Reasoning": 5.345291479820627,
      "Information/Advice seeking": 5.435643564356436,
      "Math & Data Analysis": 4.579365079365079
    },
    "total": 1024,
    "avg_len": 3335.638671875
  },
  "claude-3-opus-20240229": {
    "model": "claude-3-opus-20240229",
    "score": 7.60546875,
    "adjusted_score": 5.2109375,
    "task_macro_score": 5.171404760028754,
    "adjusted_task_macro_score": 5.171404760028754,
    "task_categorized_scores": {
      "Coding & Debugging": 5.330188679245284,
      "Creative Tasks": 5.302325581395349,
      "Planning & Reasoning": 5.252615844544096,
      "Information/Advice seeking": 5.346534653465346,
      "Math & Data Analysis": 4.674603174603174
    },
    "total": 1024,
    "avg_len": 2685.9794921875
  },
  "deepseekv2-chat": {
    "model": "deepseekv2-chat",
    "score": 7.502443792766374,
    "adjusted_score": 5.004887585532748,
    "task_macro_score": 4.821191935259587,
    "adjusted_task_macro_score": 4.821191935259587,
    "task_categorized_scores": {
      "Coding & Debugging": 4.443396226415095,
      "Creative Tasks": 5.359173126614987,
      "Planning & Reasoning": 5.062874251497005,
      "Information/Advice seeking": 5.181141439205955,
      "Math & Data Analysis": 4.4523809523809526
    },
    "total": 1023,
    "avg_len": 2896.965786901271
  },
  "Meta-Llama-3-70B-Instruct": {
    "model": "Meta-Llama-3-70B-Instruct",
    "score": 7.478983382209188,
    "adjusted_score": 4.9579667644183765,
    "task_macro_score": 4.777080449630633,
    "adjusted_task_macro_score": 4.777080449630633,
    "task_categorized_scores": {
      "Coding & Debugging": 4.471698113207546,
      "Creative Tasks": 5.430051813471502,
      "Planning & Reasoning": 5.0074738415545585,
      "Information/Advice seeking": 5.227722772277227,
      "Math & Data Analysis": 4.206349206349206
    },
    "total": 1023,
    "avg_len": 3046.6383186705766
  },
  "gemma-2-27b-it@together": {
    "model": "gemma-2-27b-it@together",
    "score": 7.4697265625,
    "adjusted_score": 4.939453125,
    "task_macro_score": 4.854019672452688,
    "adjusted_task_macro_score": 4.854019672452688,
    "task_categorized_scores": {
      "Planning & Reasoning": 5.055472263868065,
      "Information/Advice seeking": 5.049504950495049,
      "Coding & Debugging": 4.701421800947868,
      "Creative Tasks": 5.362694300518134,
      "Math & Data Analysis": 4.3919999999999995
    },
    "total": 1024,
    "avg_len": 2924.5455435847207
  },
  "yi-large": {
    "model": "yi-large",
    "score": 7.446725317693059,
    "adjusted_score": 4.8934506353861185,
    "task_macro_score": 4.892726960200772,
    "adjusted_task_macro_score": 4.892726960200772,
    "task_categorized_scores": {
      "Planning & Reasoning": 5.133834586466165,
      "Information/Advice seeking": 5.096774193548388,
      "Coding & Debugging": 4.771428571428572,
      "Creative Tasks": 5.180156657963446,
      "Math & Data Analysis": 4.446215139442231
    },
    "total": 1023,
    "avg_len": 3095.335952848723
  },
  "deepseek-coder-v2": {
    "model": "deepseek-coder-v2",
    "score": 7.4447702834799605,
    "adjusted_score": 4.889540566959921,
    "task_macro_score": 4.739521235239142,
    "adjusted_task_macro_score": 4.739521235239142,
    "task_categorized_scores": {
      "Coding & Debugging": 4.485714285714286,
      "Creative Tasks": 5.449350649350649,
      "Planning & Reasoning": 4.924698795180722,
      "Information/Advice seeking": 5.154228855721392,
      "Math & Data Analysis": 4.159362549800797
    },
    "total": 1023,
    "avg_len": 2795.3091265947005
  },
  "nemotron-4-340b-instruct": {
    "model": "nemotron-4-340b-instruct",
    "score": 7.4423828125,
    "adjusted_score": 4.884765625,
    "task_macro_score": 4.767250981186394,
    "adjusted_task_macro_score": 4.767250981186394,
    "task_categorized_scores": {
      "Planning & Reasoning": 4.912912912912914,
      "Information/Advice seeking": 5.300248138957816,
      "Coding & Debugging": 4.625592417061611,
      "Creative Tasks": 5.33160621761658,
      "Math & Data Analysis": 4.0803212851405615
    },
    "total": 1024,
    "avg_len": 2754.0098039215686
  },
  "gemini-1.5-pro": {
    "model": "gemini-1.5-pro",
    "score": 7.369140625,
    "adjusted_score": 4.73828125,
    "task_macro_score": 5.295184246265066,
    "adjusted_task_macro_score": 5.295184246265066,
    "task_categorized_scores": {
      "Planning & Reasoning": 5.373271889400922,
      "Information/Advice seeking": 5.222506393861893,
      "Coding & Debugging": 5.522388059701493,
      "Creative Tasks": 5.512465373961218,
      "Math & Data Analysis": 4.859437751004016
    },
    "total": 1024,
    "avg_len": 3247.9673135852913
  },
  "Yi-1.5-34B-Chat": {
    "model": "Yi-1.5-34B-Chat",
    "score": 7.367546432062561,
    "adjusted_score": 4.7350928641251215,
    "task_macro_score": 4.561346347759096,
    "adjusted_task_macro_score": 4.561346347759096,
    "task_categorized_scores": {
      "Planning & Reasoning": 4.8108108108108105,
      "Information/Advice seeking": 5.029702970297029,
      "Coding & Debugging": 4.208530805687204,
      "Creative Tasks": 5.352331606217616,
      "Math & Data Analysis": 3.9437751004016057
    },
    "total": 1023,
    "avg_len": 3523.557843137255
  },
  "Mistral-Nemo-Instruct-2407": {
    "model": "Mistral-Nemo-Instruct-2407",
    "score": 7.343108504398827,
    "adjusted_score": 4.686217008797653,
    "task_macro_score": 4.437513167010813,
    "adjusted_task_macro_score": 4.437513167010813,
    "task_categorized_scores": {
      "Coding & Debugging": 3.971563981042655,
      "Creative Tasks": 5.457364341085272,
      "Planning & Reasoning": 4.741405082212257,
      "Information/Advice seeking": 5.193069306930694,
      "Math & Data Analysis": 3.5634920634920633
    },
    "total": 1023,
    "avg_len": 3318.2130987292276
  },
  "Qwen2-72B-Instruct": {
    "model": "Qwen2-72B-Instruct",
    "score": 7.3203125,
    "adjusted_score": 4.640625,
    "task_macro_score": 4.44976912962341,
    "adjusted_task_macro_score": 4.44976912962341,
    "task_categorized_scores": {
      "Coding & Debugging": 3.981132075471699,
      "Creative Tasks": 4.992248062015504,
      "Planning & Reasoning": 4.684603886397609,
      "Information/Advice seeking": 4.950495049504951,
      "Math & Data Analysis": 4.095238095238095
    },
    "total": 1024,
    "avg_len": 2856.4482421875
  },
  "gemma-2-9b-it": {
    "model": "gemma-2-9b-it",
    "score": 7.268101761252447,
    "adjusted_score": 4.536203522504893,
    "task_macro_score": 4.2696193124381026,
    "adjusted_task_macro_score": 4.2696193124381026,
    "task_categorized_scores": {
      "Coding & Debugging": 3.666666666666666,
      "Creative Tasks": 5.10077519379845,
      "Planning & Reasoning": 4.665667166416792,
      "Information/Advice seeking": 4.896039603960396,
      "Math & Data Analysis": 3.6428571428571423
    },
    "total": 1022,
    "avg_len": 2802.8923679060667
  },
  "claude-3-sonnet-20240229": {
    "model": "claude-3-sonnet-20240229",
    "score": 7.262230919765166,
    "adjusted_score": 4.524461839530332,
    "task_macro_score": 4.548145776375293,
    "adjusted_task_macro_score": 4.548145776375293,
    "task_categorized_scores": {
      "Coding & Debugging": 4.609523809523809,
      "Creative Tasks": 4.630490956072352,
      "Planning & Reasoning": 4.742514970059879,
      "Information/Advice seeking": 4.7128712871287135,
      "Math & Data Analysis": 4.063745019920319
    },
    "total": 1022,
    "avg_len": 2670.243639921722
  },
  "gemini-1.5-flash": {
    "model": "gemini-1.5-flash",
    "score": 7.2074363992172215,
    "adjusted_score": 4.414872798434443,
    "task_macro_score": 4.885062170599163,
    "adjusted_task_macro_score": 4.885062170599163,
    "task_categorized_scores": {
      "Planning & Reasoning": 5.078582434514638,
      "Information/Advice seeking": 4.866666666666667,
      "Coding & Debugging": 4.872549019607844,
      "Creative Tasks": 5.165745856353592,
      "Math & Data Analysis": 4.53225806451613
    },
    "total": 1022,
    "avg_len": 3654.3993871297243
  },
  "Qwen1.5-72B-Chat-greedy": {
    "model": "Qwen1.5-72B-Chat-greedy",
    "score": 7.173359451518119,
    "adjusted_score": 4.346718903036239,
    "task_macro_score": 3.992771366582465,
    "adjusted_task_macro_score": 3.992771366582465,
    "task_categorized_scores": {
      "Coding & Debugging": 3.5355450236966828,
      "Creative Tasks": 5.036269430051814,
      "Planning & Reasoning": 4.345345345345345,
      "Information/Advice seeking": 4.821782178217822,
      "Math & Data Analysis": 2.9800796812748995
    },
    "total": 1021,
    "avg_len": 2392.364348677767
  },
  "deepseek-v2-coder-0628": {
    "model": "deepseek-v2-coder-0628",
    "score": 7.171875,
    "adjusted_score": 4.34375,
    "task_macro_score": 4.566459211926647,
    "adjusted_task_macro_score": 4.566459211926647,
    "task_categorized_scores": {
      "Coding & Debugging": 4.886792452830189,
      "Creative Tasks": 4.077519379844961,
      "Planning & Reasoning": 4.7174887892376685,
      "Information/Advice seeking": 4.0049504950495045,
      "Math & Data Analysis": 4.642857142857142
    },
    "total": 1024,
    "avg_len": 2580.181640625
  },
  "Llama-3-8B-Magpie-Align-v0.1": {
    "model": "Llama-3-8B-Magpie-Align-v0.1",
    "score": 7.1223091976516635,
    "adjusted_score": 4.244618395303327,
    "task_macro_score": 3.9290196827463255,
    "adjusted_task_macro_score": 3.9290196827463255,
    "task_categorized_scores": {
      "Coding & Debugging": 3.374407582938389,
      "Creative Tasks": 4.919896640826874,
      "Planning & Reasoning": 4.27245508982036,
      "Information/Advice seeking": 4.891089108910892,
      "Math & Data Analysis": 2.976000000000001
    },
    "total": 1022,
    "avg_len": 3107.77397260274
  },
  "mistral-large-2402": {
    "model": "mistral-large-2402",
    "score": 7.114369501466276,
    "adjusted_score": 4.228739002932551,
    "task_macro_score": 3.889367833445423,
    "adjusted_task_macro_score": 3.889367833445423,
    "task_categorized_scores": {
      "Coding & Debugging": 3.374407582938389,
      "Creative Tasks": 4.966408268733851,
      "Planning & Reasoning": 4.179910044977511,
      "Information/Advice seeking": 4.613861386138614,
      "Math & Data Analysis": 3.087999999999999
    },
    "total": 1023,
    "avg_len": 2514.9814090019568
  },
  "command-r-plus": {
    "model": "command-r-plus",
    "score": 7.078277886497065,
    "adjusted_score": 4.15655577299413,
    "task_macro_score": 3.676236856767293,
    "adjusted_task_macro_score": 3.676236856767293,
    "task_categorized_scores": {
      "Coding & Debugging": 2.843601895734597,
      "Creative Tasks": 5.2558139534883725,
      "Planning & Reasoning": 4.194902548725636,
      "Information/Advice seeking": 4.915841584158416,
      "Math & Data Analysis": 2.3492063492063497
    },
    "total": 1022,
    "avg_len": 3293.812133072407
  },
  "Llama-3-Instruct-8B-SimPO-v0.2": {
    "model": "Llama-3-Instruct-8B-SimPO-v0.2",
    "score": 7.075268817204301,
    "adjusted_score": 4.150537634408602,
    "task_macro_score": 3.7155419825936797,
    "adjusted_task_macro_score": 3.7155419825936797,
    "task_categorized_scores": {
      "Coding & Debugging": 3.150943396226415,
      "Creative Tasks": 5.183462532299741,
      "Planning & Reasoning": 4.071856287425149,
      "Information/Advice seeking": 4.7871287128712865,
      "Math & Data Analysis": 2.438247011952191
    },
    "total": 1023,
    "avg_len": 2533.764418377322
  },
  "Llama-3-Instruct-8B-SimPO": {
    "model": "Llama-3-Instruct-8B-SimPO",
    "score": 7.058651026392962,
    "adjusted_score": 4.117302052785924,
    "task_macro_score": 3.7049721402304923,
    "adjusted_task_macro_score": 3.7049721402304923,
    "task_categorized_scores": {
      "Coding & Debugging": 3.1753554502369674,
      "Creative Tasks": 5.064599483204134,
      "Planning & Reasoning": 4.086696562032884,
      "Information/Advice seeking": 4.7871287128712865,
      "Math & Data Analysis": 2.3984063745019917
    },
    "total": 1023,
    "avg_len": 2541.9257086999023
  },
  "glm-4-9b-chat": {
    "model": "glm-4-9b-chat",
    "score": 7.058651026392962,
    "adjusted_score": 4.117302052785924,
    "task_macro_score": 3.909896797431742,
    "adjusted_task_macro_score": 3.909896797431742,
    "task_categorized_scores": {
      "Coding & Debugging": 3.537735849056604,
      "Creative Tasks": 4.775193798449612,
      "Planning & Reasoning": 4.248502994011975,
      "Information/Advice seeking": 4.628712871287128,
      "Math & Data Analysis": 2.9800796812748995
    },
    "total": 1023,
    "avg_len": 3692.043010752688
  },
  "reka-core-20240501": {
    "model": "reka-core-20240501",
    "score": 7.0517578125,
    "adjusted_score": 4.103515625,
    "task_macro_score": 4.590279465292558,
    "adjusted_task_macro_score": 4.590279465292558,
    "task_categorized_scores": {
      "Planning & Reasoning": 4.800632911392405,
      "Information/Advice seeking": 5.225464190981432,
      "Coding & Debugging": 4.060301507537689,
      "Creative Tasks": 5.548746518105849,
      "Math & Data Analysis": 4.034188034188034
    },
    "total": 1024,
    "avg_len": 2592.589397089397
  },
  "claude-3-haiku-20240307": {
    "model": "claude-3-haiku-20240307",
    "score": 7.0126953125,
    "adjusted_score": 4.025390625,
    "task_macro_score": 3.8893606666167266,
    "adjusted_task_macro_score": 3.8893606666167266,
    "task_categorized_scores": {
      "Coding & Debugging": 3.69811320754717,
      "Creative Tasks": 4.294573643410853,
      "Planning & Reasoning": 4.128550074738415,
      "Information/Advice seeking": 4.534653465346535,
      "Math & Data Analysis": 3.1428571428571423
    },
    "total": 1024,
    "avg_len": 2601.029296875
  },
  "SELM-Llama-3-8B-Instruct-iter-3": {
    "model": "SELM-Llama-3-8B-Instruct-iter-3",
    "score": 6.9980392156862745,
    "adjusted_score": 3.996078431372549,
    "task_macro_score": 3.525906077680738,
    "adjusted_task_macro_score": 3.525906077680738,
    "task_categorized_scores": {
      "Coding & Debugging": 2.7333333333333325,
      "Creative Tasks": 5.105943152454781,
      "Planning & Reasoning": 3.9789789789789793,
      "Information/Advice seeking": 4.605459057071961,
      "Math & Data Analysis": 2.3505976095617527
    },
    "total": 1020,
    "avg_len": 2913.1470588235293
  },
  "Yi-1.5-9B-Chat": {
    "model": "Yi-1.5-9B-Chat",
    "score": 6.992179863147605,
    "adjusted_score": 3.98435972629521,
    "task_macro_score": 3.8665353515172316,
    "adjusted_task_macro_score": 3.8665353515172316,
    "task_categorized_scores": {
      "Planning & Reasoning": 4.237237237237236,
      "Information/Advice seeking": 4.262376237623762,
      "Coding & Debugging": 3.4976303317535553,
      "Creative Tasks": 4.5595854922279795,
      "Math & Data Analysis": 3.2208835341365454
    },
    "total": 1023,
    "avg_len": 3468.23431372549
  },
  "Llama-3-Instruct-8B-SimPO-ExPO": {
    "model": "Llama-3-Instruct-8B-SimPO-ExPO",
    "score": 6.98435972629521,
    "adjusted_score": 3.9687194525904204,
    "task_macro_score": 3.501502977266739,
    "adjusted_task_macro_score": 3.501502977266739,
    "task_categorized_scores": {
      "Coding & Debugging": 2.8584905660377355,
      "Creative Tasks": 4.9147286821705425,
      "Planning & Reasoning": 3.9461077844311383,
      "Information/Advice seeking": 4.732673267326733,
      "Math & Data Analysis": 2.1195219123505975
    },
    "total": 1023,
    "avg_len": 2480.6490713587486
  },
  "dbrx-instruct@together": {
    "model": "dbrx-instruct@together",
    "score": 6.777126099706745,
    "adjusted_score": 3.55425219941349,
    "task_macro_score": 3.2598891595850845,
    "adjusted_task_macro_score": 3.2598891595850845,
    "task_categorized_scores": {
      "Coding & Debugging": 2.644549763033176,
      "Creative Tasks": 4.232558139534884,
      "Planning & Reasoning": 3.6227544910179645,
      "Information/Advice seeking": 4.108910891089108,
      "Math & Data Analysis": 2.4523809523809526
    },
    "total": 1023,
    "avg_len": 2576.5190615835777
  },
  "command-r": {
    "model": "command-r",
    "score": 6.7529296875,
    "adjusted_score": 3.505859375,
    "task_macro_score": 2.9533143228506247,
    "adjusted_task_macro_score": 2.9533143228506247,
    "task_categorized_scores": {
      "Coding & Debugging": 1.933962264150944,
      "Creative Tasks": 4.7441860465116275,
      "Planning & Reasoning": 3.461883408071749,
      "Information/Advice seeking": 4.410891089108912,
      "Math & Data Analysis": 1.6031746031746028
    },
    "total": 1024,
    "avg_len": 2919.423828125
  },
  "Mixtral-8x7B-Instruct-v0.1": {
    "model": "Mixtral-8x7B-Instruct-v0.1",
    "score": 6.75146771037182,
    "adjusted_score": 3.50293542074364,
    "task_macro_score": 3.147027304895869,
    "adjusted_task_macro_score": 3.147027304895869,
    "task_categorized_scores": {
      "Coding & Debugging": 2.5023696682464447,
      "Creative Tasks": 4.275324675324676,
      "Planning & Reasoning": 3.458646616541353,
      "Information/Advice seeking": 4.193548387096774,
      "Math & Data Analysis": 2.2142857142857135
    },
    "total": 1022,
    "avg_len": 2653.5813725490198
  },
  "Starling-LM-7B-beta-ExPO": {
    "model": "Starling-LM-7B-beta-ExPO",
    "score": 6.750733137829912,
    "adjusted_score": 3.5014662756598245,
    "task_macro_score": 3.1559353823619887,
    "adjusted_task_macro_score": 3.1559353823619887,
    "task_categorized_scores": {
      "Planning & Reasoning": 3.631736526946108,
      "Information/Advice seeking": 4.2871287128712865,
      "Coding & Debugging": 2.5308056872037916,
      "Creative Tasks": 4.430051813471502,
      "Math & Data Analysis": 1.8571428571428577
    },
    "total": 1023,
    "avg_len": 2835.826810176125
  },
  "reka-flash-20240226": {
    "model": "reka-flash-20240226",
    "score": 6.730205278592376,
    "adjusted_score": 3.460410557184751,
    "task_macro_score": 3.0363615402031146,
    "adjusted_task_macro_score": 3.0363615402031146,
    "task_categorized_scores": {
      "Planning & Reasoning": 3.501501501501501,
      "Information/Advice seeking": 4.153465346534654,
      "Coding & Debugging": 2.2085308056872037,
      "Creative Tasks": 4.244155844155845,
      "Math & Data Analysis": 2.048
    },
    "total": 1023,
    "avg_len": 2103.0098039215686
  },
  "Starling-LM-7B-beta": {
    "model": "Starling-LM-7B-beta",
    "score": 6.70869990224829,
    "adjusted_score": 3.417399804496579,
    "task_macro_score": 3.0169449808290145,
    "adjusted_task_macro_score": 3.0169449808290145,
    "task_categorized_scores": {
      "Planning & Reasoning": 3.405082212257101,
      "Information/Advice seeking": 4.188118811881187,
      "Coding & Debugging": 2.436018957345972,
      "Creative Tasks": 4.379220779220779,
      "Math & Data Analysis": 1.6984126984126977
    },
    "total": 1023,
    "avg_len": 2797.807240704501
  },
  "Nous-Hermes-2-Mixtral-8x7B-DPO": {
    "model": "Nous-Hermes-2-Mixtral-8x7B-DPO",
    "score": 6.6611165523996085,
    "adjusted_score": 3.322233104799217,
    "task_macro_score": 3.071140030667612,
    "adjusted_task_macro_score": 3.071140030667612,
    "task_categorized_scores": {
      "Coding & Debugging": 2.6037735849056602,
      "Creative Tasks": 3.792207792207792,
      "Planning & Reasoning": 3.424287856071963,
      "Information/Advice seeking": 3.9752475247524757,
      "Math & Data Analysis": 2.1752988047808763
    },
    "total": 1021,
    "avg_len": 2874.541625857003
  },
  "Meta-Llama-3-8B-Instruct": {
    "model": "Meta-Llama-3-8B-Instruct",
    "score": 6.658846529814272,
    "adjusted_score": 3.317693059628544,
    "task_macro_score": 2.920277208638918,
    "adjusted_task_macro_score": 2.920277208638918,
    "task_categorized_scores": {
      "Coding & Debugging": 2.19811320754717,
      "Creative Tasks": 4.356589147286822,
      "Planning & Reasoning": 3.4401197604790426,
      "Information/Advice seeking": 3.9306930693069315,
      "Math & Data Analysis": 1.6972111553784863
    },
    "total": 1023,
    "avg_len": 2975.1876832844573
  },
  "Hermes-2-Theta-Llama-3-8B": {
    "model": "Hermes-2-Theta-Llama-3-8B",
    "score": 6.64711632453568,
    "adjusted_score": 3.2942326490713594,
    "task_macro_score": 2.9635207776375476,
    "adjusted_task_macro_score": 2.9635207776375476,
    "task_categorized_scores": {
      "Coding & Debugging": 2.3113207547169807,
      "Creative Tasks": 3.9793281653746764,
      "Planning & Reasoning": 3.365269461077844,
      "Information/Advice seeking": 4.158415841584159,
      "Math & Data Analysis": 1.8725099601593627
    },
    "total": 1023,
    "avg_len": 2742.169110459433
  },
  "tulu-2-dpo-70b": {
    "model": "tulu-2-dpo-70b",
    "score": 6.6412512218963835,
    "adjusted_score": 3.282502443792767,
    "task_macro_score": 2.7983756123225105,
    "adjusted_task_macro_score": 2.7983756123225105,
    "task_categorized_scores": {
      "Planning & Reasoning": 3.230538922155688,
      "Information/Advice seeking": 4.0693069306930685,
      "Coding & Debugging": 2.0663507109004744,
      "Creative Tasks": 4.270129870129869,
      "Math & Data Analysis": 1.4841269841269842
    },
    "total": 1023,
    "avg_len": 2908.0714285714284
  },
  "gemma-2-2b-it": {
    "model": "gemma-2-2b-it",
    "score": 6.636007827788649,
    "adjusted_score": 3.272015655577299,
    "task_macro_score": 2.7826043214654264,
    "adjusted_task_macro_score": 2.7826043214654264,
    "task_categorized_scores": {
      "Coding & Debugging": 1.7904761904761912,
      "Creative Tasks": 4.361757105943152,
      "Planning & Reasoning": 3.3811659192825108,
      "Information/Advice seeking": 3.990099009900991,
      "Math & Data Analysis": 1.579365079365079
    },
    "total": 1022,
    "avg_len": 3589.3894324853227
  },
  "gpt-3.5-turbo-0125": {
    "model": "gpt-3.5-turbo-0125",
    "score": 6.613880742913001,
    "adjusted_score": 3.2277614858260026,
    "task_macro_score": 3.0015986071959313,
    "adjusted_task_macro_score": 3.0015986071959313,
    "task_categorized_scores": {
      "Coding & Debugging": 2.654028436018958,
      "Creative Tasks": 3.7416020671834627,
      "Planning & Reasoning": 3.3393124065769797,
      "Information/Advice seeking": 3.6485148514851478,
      "Math & Data Analysis": 2.158730158730158
    },
    "total": 1023,
    "avg_len": 1844.13880742913
  },
  "SELM-Zephyr-7B-iter-3": {
    "model": "SELM-Zephyr-7B-iter-3",
    "score": 6.576171875,
    "adjusted_score": 3.15234375,
    "task_macro_score": 2.5061899136983596,
    "adjusted_task_macro_score": 2.5061899136983596,
    "task_categorized_scores": {
      "Coding & Debugging": 1.1037735849056602,
      "Creative Tasks": 4.470284237726098,
      "Planning & Reasoning": 3.158682634730539,
      "Information/Advice seeking": 4.099009900990099,
      "Math & Data Analysis": 1.2669322709163353
    },
    "total": 1024,
    "avg_len": 2823.7800586510266
  },
  "Mistral-7B-Instruct-v0.2": {
    "model": "Mistral-7B-Instruct-v0.2",
    "score": 6.534701857282503,
    "adjusted_score": 3.0694037145650057,
    "task_macro_score": 2.563372831895388,
    "adjusted_task_macro_score": 2.563372831895388,
    "task_categorized_scores": {
      "Coding & Debugging": 1.8396226415094343,
      "Creative Tasks": 4.207253886010363,
      "Planning & Reasoning": 3.0059880239520957,
      "Information/Advice seeking": 4.009925558312656,
      "Math & Data Analysis": 1.007936507936508
    },
    "total": 1023,
    "avg_len": 2832.3440860215055
  },
  "neo_7b_instruct_v0.1": {
    "model": "neo_7b_instruct_v0.1",
    "score": 6.4599609375,
    "adjusted_score": 2.919921875,
    "task_macro_score": 2.5019233576987165,
    "adjusted_task_macro_score": 2.5019233576987165,
    "task_categorized_scores": {
      "Planning & Reasoning": 3.144992526158445,
      "Information/Advice seeking": 3.6336633663366342,
      "Coding & Debugging": 1.402843601895734,
      "Creative Tasks": 3.948186528497409,
      "Math & Data Analysis": 1.5
    },
    "total": 1024,
    "avg_len": 3735.800586510264
  },
  "neo_7b_instruct_v0.1-ExPO": {
    "model": "neo_7b_instruct_v0.1-ExPO",
    "score": 6.381231671554252,
    "adjusted_score": 2.7624633431085037,
    "task_macro_score": 2.3114172189706186,
    "adjusted_task_macro_score": 2.3114172189706186,
    "task_categorized_scores": {
      "Planning & Reasoning": 2.8669656203288483,
      "Information/Advice seeking": 3.4851485148514847,
      "Coding & Debugging": 1.276190476190477,
      "Creative Tasks": 3.8549222797927456,
      "Math & Data Analysis": 1.2589641434262955
    },
    "total": 1023,
    "avg_len": 4107.917808219178
  },
  "Qwen1.5-7B-Chat@together": {
    "model": "Qwen1.5-7B-Chat@together",
    "score": 6.36852394916911,
    "adjusted_score": 2.7370478983382203,
    "task_macro_score": 2.342316313940188,
    "adjusted_task_macro_score": 2.342316313940188,
    "task_categorized_scores": {
      "Coding & Debugging": 1.488151658767773,
      "Creative Tasks": 3.829457364341085,
      "Planning & Reasoning": 2.8878923766816147,
      "Information/Advice seeking": 3.400990099009901,
      "Math & Data Analysis": 1.1904761904761898
    },
    "total": 1023,
    "avg_len": 2519.4203323558163
  },
  "Llama-2-70b-chat-hf": {
    "model": "Llama-2-70b-chat-hf",
    "score": 6.345703125,
    "adjusted_score": 2.69140625,
    "task_macro_score": 2.065963691286665,
    "adjusted_task_macro_score": 2.065963691286665,
    "task_categorized_scores": {
      "Planning & Reasoning": 2.684684684684685,
      "Information/Advice seeking": 3.830845771144279,
      "Coding & Debugging": 0.9333333333333336,
      "Creative Tasks": 4.0,
      "Math & Data Analysis": 0.41767068273092356
    },
    "total": 1024,
    "avg_len": 3138.3179587831205
  },
  "ul1_15": {
    "model": "ul1_15",
    "score": 6.333333333333333,
    "adjusted_score": 2.666666666666666,
    "task_macro_score": 2.6315789473684212,
    "adjusted_task_macro_score": 2.6315789473684212,
    "task_categorized_scores": {
      "Coding & Debugging": 2.0,
      "Creative Tasks": 2.0,
      "Planning & Reasoning": 4.0,
      "Math & Data Analysis": 4.0
    },
    "total": 3,
    "avg_len": 5430.666666666667
  },
  "Yi-1.5-6B-Chat": {
    "model": "Yi-1.5-6B-Chat",
    "score": 6.263929618768328,
    "adjusted_score": 2.5278592375366564,
    "task_macro_score": 2.331811668914988,
    "adjusted_task_macro_score": 2.331811668914988,
    "task_categorized_scores": {
      "Planning & Reasoning": 2.72972972972973,
      "Information/Advice seeking": 3.1414392059553347,
      "Coding & Debugging": 1.6587677725118475,
      "Creative Tasks": 3.108808290155441,
      "Math & Data Analysis": 1.6799999999999997
    },
    "total": 1023,
    "avg_len": 3899.4686274509804
  },
  "reka-edge": {
    "model": "reka-edge",
    "score": 6.159335288367546,
    "adjusted_score": 2.3186705767350926,
    "task_macro_score": 2.125225793299967,
    "adjusted_task_macro_score": 2.125225793299967,
    "task_categorized_scores": {
      "Planning & Reasoning": 2.5007727975270484,
      "Information/Advice seeking": 3.4389610389610397,
      "Coding & Debugging": 1.3526570048309186,
      "Creative Tasks": 3.618037135278515,
      "Math & Data Analysis": 0.8897959183673461
    },
    "total": 1023,
    "avg_len": 2417.351106639839
  },
  "Llama-2-7b-chat-hf": {
    "model": "Llama-2-7b-chat-hf",
    "score": 5.761252446183953,
    "adjusted_score": 1.5225048923679054,
    "task_macro_score": 0.8262075264042466,
    "adjusted_task_macro_score": 0.8262075264042466,
    "task_categorized_scores": {
      "Planning & Reasoning": 1.5428571428571427,
      "Information/Advice seeking": 2.766169154228855,
      "Coding & Debugging": -0.6794258373205739,
      "Creative Tasks": 2.976623376623376,
      "Math & Data Analysis": -0.7177419354838701
    },
    "total": 1022,
    "avg_len": 2985.1052114060963
  },
  "adv_1_165": {
    "model": "adv_1_165",
    "score": 5.578125,
    "adjusted_score": 1.15625,
    "task_macro_score": 0.586293364708039,
    "adjusted_task_macro_score": 0.586293364708039,
    "task_categorized_scores": {
      "Creative Tasks": 2.195652173913043,
      "Math & Data Analysis": -1.23076923076923,
      "Planning & Reasoning": 1.2542372881355934,
      "Information/Advice seeking": 1.7346938775510203,
      "Coding & Debugging": 0.039215686274509665
    },
    "total": 256,
    "avg_len": 2971.6156862745097
  },
  "gemma-7b-it": {
    "model": "gemma-7b-it",
    "score": 5.5087890625,
    "adjusted_score": 1.017578125,
    "task_macro_score": 0.661975914869064,
    "adjusted_task_macro_score": 0.661975914869064,
    "task_categorized_scores": {
      "Planning & Reasoning": 1.0164424514200299,
      "Information/Advice seeking": 1.272277227722773,
      "Coding & Debugging": 0.18009478672985857,
      "Creative Tasks": 2.119170984455959,
      "Math & Data Analysis": -0.36507936507936556
    },
    "total": 1024,
    "avg_len": 1726.3440860215053
  },
  "adv_3_465": {
    "model": "adv_3_465",
    "score": 5.46875,
    "adjusted_score": 0.9375,
    "task_macro_score": 0.3811584730004343,
    "adjusted_task_macro_score": 0.3811584730004343,
    "task_categorized_scores": {
      "Coding & Debugging": -0.615384615384615,
      "Creative Tasks": 2.10752688172043,
      "Math & Data Analysis": -0.8307692307692314,
      "Planning & Reasoning": 1.028248587570621,
      "Information/Advice seeking": 1.4285714285714288
    },
    "total": 256,
    "avg_len": 1877.9140625
  },
  "adv_3_90": {
    "model": "adv_3_90",
    "score": 5.46484375,
    "adjusted_score": 0.9296875,
    "task_macro_score": 0.4371323242586967,
    "adjusted_task_macro_score": 0.4371323242586967,
    "task_categorized_scores": {
      "Coding & Debugging": -0.26923076923077005,
      "Creative Tasks": 1.46236559139785,
      "Math & Data Analysis": -0.9846153846153847,
      "Planning & Reasoning": 1.1412429378531073,
      "Information/Advice seeking": 1.6530612244897966
    },
    "total": 256,
    "avg_len": 2411.0625
  },
  "adv_3_165": {
    "model": "adv_3_165",
    "score": 5.409448818897638,
    "adjusted_score": 0.8188976377952759,
    "task_macro_score": 0.29333267852909656,
    "adjusted_task_macro_score": 0.29333267852909656,
    "task_categorized_scores": {
      "Coding & Debugging": -0.4399999999999995,
      "Creative Tasks": 1.591397849462366,
      "Math & Data Analysis": -1.09375,
      "Planning & Reasoning": 0.9604519774011298,
      "Information/Advice seeking": 1.387755102040817
    },
    "total": 254,
    "avg_len": 2528.8740157480315
  },
  "adv_2_465": {
    "model": "adv_2_465",
    "score": 5.37890625,
    "adjusted_score": 0.7578125,
    "task_macro_score": 0.33285334803474065,
    "adjusted_task_macro_score": 0.33285334803474065,
    "task_categorized_scores": {
      "Creative Tasks": 1.9130434782608692,
      "Math & Data Analysis": -0.9846153846153847,
      "Planning & Reasoning": 0.9717514124293789,
      "Information/Advice seeking": 1.2448979591836729,
      "Coding & Debugging": -0.4313725490196081
    },
    "total": 256,
    "avg_len": 1942.6039215686274
  },
  "adv_3_390": {
    "model": "adv_3_390",
    "score": 5.36328125,
    "adjusted_score": 0.7265625,
    "task_macro_score": 0.35403408273747033,
    "adjusted_task_macro_score": 0.35403408273747033,
    "task_categorized_scores": {
      "Coding & Debugging": -0.5384615384615383,
      "Creative Tasks": 1.763440860215054,
      "Math & Data Analysis": -0.8307692307692314,
      "Planning & Reasoning": 0.9491525423728806,
      "Information/Advice seeking": 1.4897959183673475
    },
    "total": 256,
    "avg_len": 2100.68359375
  },
  "adv_2_315": {
    "model": "adv_2_315",
    "score": 5.3515625,
    "adjusted_score": 0.703125,
    "task_macro_score": 0.14114863718677365,
    "adjusted_task_macro_score": 0.14114863718677365,
    "task_categorized_scores": {
      "Creative Tasks": 1.9565217391304355,
      "Math & Data Analysis": -1.1692307692307686,
      "Planning & Reasoning": 0.6553672316384187,
      "Information/Advice seeking": 1.1020408163265305,
      "Coding & Debugging": -0.6274509803921564
    },
    "total": 256,
    "avg_len": 1891.6392156862746
  },
  "adv_1_315": {
    "model": "adv_1_315",
    "score": 5.34765625,
    "adjusted_score": 0.6953125,
    "task_macro_score": 0.28624373781149304,
    "adjusted_task_macro_score": 0.28624373781149304,
    "task_categorized_scores": {
      "Creative Tasks": 1.9347826086956523,
      "Math & Data Analysis": -0.861538461538462,
      "Planning & Reasoning": 0.9717514124293789,
      "Information/Advice seeking": 1.1632653061224492,
      "Coding & Debugging": -0.6666666666666661
    },
    "total": 256,
    "avg_len": 1979.6313725490197
  },
  "adv_3_315": {
    "model": "adv_3_315",
    "score": 5.34375,
    "adjusted_score": 0.6875,
    "task_macro_score": 0.23113899752475406,
    "adjusted_task_macro_score": 0.23113899752475406,
    "task_categorized_scores": {
      "Coding & Debugging": -0.42307692307692335,
      "Creative Tasks": 1.806451612903226,
      "Math & Data Analysis": -0.9846153846153847,
      "Planning & Reasoning": 0.6440677966101696,
      "Information/Advice seeking": 1.204081632653061
    },
    "total": 256,
    "avg_len": 1984.375
  },
  "adv_1_615": {
    "model": "adv_1_615",
    "score": 5.3203125,
    "adjusted_score": 0.640625,
    "task_macro_score": 0.06290009217760814,
    "adjusted_task_macro_score": 0.06290009217760814,
    "task_categorized_scores": {
      "Creative Tasks": 1.9565217391304355,
      "Math & Data Analysis": -1.1076923076923073,
      "Planning & Reasoning": 0.8700564971751419,
      "Information/Advice seeking": 1.0816326530612237,
      "Coding & Debugging": -1.1764705882352935
    },
    "total": 256,
    "avg_len": 1929.9607843137255
  },
  "adv_2_615": {
    "model": "adv_2_615",
    "score": 5.3203125,
    "adjusted_score": 0.640625,
    "task_macro_score": 0.2669757669431416,
    "adjusted_task_macro_score": 0.2669757669431416,
    "task_categorized_scores": {
      "Creative Tasks": 1.7391304347826093,
      "Math & Data Analysis": -0.8923076923076927,
      "Planning & Reasoning": 0.8248587570621471,
      "Information/Advice seeking": 1.0,
      "Coding & Debugging": -0.3921568627450984
    },
    "total": 256,
    "avg_len": 1757.0392156862745
  },
  "adv_2_240": {
    "model": "adv_2_240",
    "score": 5.31640625,
    "adjusted_score": 0.6328125,
    "task_macro_score": 0.13810887817635178,
    "adjusted_task_macro_score": 0.13810887817635178,
    "task_categorized_scores": {
      "Creative Tasks": 1.804347826086957,
      "Math & Data Analysis": -1.23076923076923,
      "Planning & Reasoning": 0.6779661016949152,
      "Information/Advice seeking": 1.1632653061224492,
      "Coding & Debugging": -0.5882352941176467
    },
    "total": 256,
    "avg_len": 1892.137254901961
  },
  "adv_1_540": {
    "model": "adv_1_540",
    "score": 5.3125,
    "adjusted_score": 0.625,
    "task_macro_score": 0.05067971319872724,
    "adjusted_task_macro_score": 0.05067971319872724,
    "task_categorized_scores": {
      "Creative Tasks": 1.8695652173913047,
      "Math & Data Analysis": -1.1076923076923073,
      "Planning & Reasoning": 0.8926553672316384,
      "Information/Advice seeking": 0.9591836734693882,
      "Coding & Debugging": -1.1372549019607838
    },
    "total": 256,
    "avg_len": 1812.2470588235294
  },
  "adv_2_15": {
    "model": "adv_2_15",
    "score": 5.27734375,
    "adjusted_score": 0.5546875,
    "task_macro_score": 0.002175150202545258,
    "adjusted_task_macro_score": 0.002175150202545258,
    "task_categorized_scores": {
      "Coding & Debugging": -0.9615384615384617,
      "Creative Tasks": 1.526881720430108,
      "Math & Data Analysis": -1.292307692307693,
      "Planning & Reasoning": 0.621468926553673,
      "Information/Advice seeking": 1.2857142857142865
    },
    "total": 256,
    "avg_len": 4086.94140625
  },
  "adv_3_615": {
    "model": "adv_3_615",
    "score": 5.2734375,
    "adjusted_score": 0.546875,
    "task_macro_score": 0.1127881678854519,
    "adjusted_task_macro_score": 0.1127881678854519,
    "task_categorized_scores": {
      "Coding & Debugging": -0.9230769230769234,
      "Creative Tasks": 1.67741935483871,
      "Math & Data Analysis": -0.707692307692307,
      "Planning & Reasoning": 0.7570621468926557,
      "Information/Advice seeking": 0.816326530612244
    },
    "total": 256,
    "avg_len": 2028.69140625
  },
  "adv_3_690": {
    "model": "adv_3_690",
    "score": 5.2734375,
    "adjusted_score": 0.546875,
    "task_macro_score": 0.06605639092473954,
    "adjusted_task_macro_score": 0.06605639092473954,
    "task_categorized_scores": {
      "Coding & Debugging": -1.26923076923077,
      "Creative Tasks": 1.720430107526882,
      "Math & Data Analysis": -0.76923076923077,
      "Planning & Reasoning": 0.8474576271186436,
      "Information/Advice seeking": 1.0
    },
    "total": 256,
    "avg_len": 1880.02734375
  },
  "adv_1_465": {
    "model": "adv_1_465",
    "score": 5.26953125,
    "adjusted_score": 0.5390625,
    "task_macro_score": -0.027660050143889853,
    "adjusted_task_macro_score": -0.027660050143889853,
    "task_categorized_scores": {
      "Creative Tasks": 1.8913043478260878,
      "Math & Data Analysis": -1.4153846153846157,
      "Planning & Reasoning": 0.7344632768361574,
      "Information/Advice seeking": 1.1224489795918373,
      "Coding & Debugging": -1.1372549019607838
    },
    "total": 256,
    "avg_len": 1798.521568627451
  },
  "adv_1_390": {
    "model": "adv_1_390",
    "score": 5.265625,
    "adjusted_score": 0.53125,
    "task_macro_score": 0.08846257862054582,
    "adjusted_task_macro_score": 0.08846257862054582,
    "task_categorized_scores": {
      "Creative Tasks": 1.9130434782608692,
      "Math & Data Analysis": -1.046153846153846,
      "Planning & Reasoning": 0.621468926553673,
      "Information/Advice seeking": 0.7551020408163271,
      "Coding & Debugging": -0.6666666666666661
    },
    "total": 256,
    "avg_len": 1990.8235294117646
  },
  "adv_3_760": {
    "model": "adv_3_760",
    "score": 5.24609375,
    "adjusted_score": 0.4921875,
    "task_macro_score": -0.020730802486723047,
    "adjusted_task_macro_score": -0.020730802486723047,
    "task_categorized_scores": {
      "Coding & Debugging": -1.384615384615385,
      "Creative Tasks": 1.591397849462366,
      "Math & Data Analysis": -0.7384615384615376,
      "Planning & Reasoning": 0.7457627118644066,
      "Information/Advice seeking": 0.8571428571428577
    },
    "total": 256,
    "avg_len": 1921.4609375
  },
  "adv_3_240": {
    "model": "adv_3_240",
    "score": 5.24609375,
    "adjusted_score": 0.4921875,
    "task_macro_score": 0.012470866782972746,
    "adjusted_task_macro_score": 0.012470866782972746,
    "task_categorized_scores": {
      "Coding & Debugging": -0.8461538461538467,
      "Creative Tasks": 1.419354838709678,
      "Math & Data Analysis": -1.3538461538461544,
      "Planning & Reasoning": 0.72316384180791,
      "Information/Advice seeking": 1.1428571428571423
    },
    "total": 256,
    "avg_len": 1760.671875
  },
  "adv_3_15": {
    "model": "adv_3_15",
    "score": 5.235294117647059,
    "adjusted_score": 0.47058823529411775,
    "task_macro_score": -0.05483145810879542,
    "adjusted_task_macro_score": -0.05483145810879542,
    "task_categorized_scores": {
      "Coding & Debugging": -1.0196078431372548,
      "Creative Tasks": 1.526881720430108,
      "Math & Data Analysis": -1.138461538461538,
      "Planning & Reasoning": 0.621468926553673,
      "Information/Advice seeking": 0.816326530612244
    },
    "total": 255,
    "avg_len": 3994.5686274509803
  },
  "adv_2_390": {
    "model": "adv_2_390",
    "score": 5.234375,
    "adjusted_score": 0.46875,
    "task_macro_score": -0.08672767150303251,
    "adjusted_task_macro_score": -0.08672767150303251,
    "task_categorized_scores": {
      "Coding & Debugging": -1.3461538461538467,
      "Creative Tasks": 1.548387096774194,
      "Math & Data Analysis": -1.1692307692307686,
      "Planning & Reasoning": 0.6101694915254239,
      "Information/Advice seeking": 1.204081632653061
    },
    "total": 256,
    "avg_len": 1846.99609375
  },
  "adv_3_540": {
    "model": "adv_3_540",
    "score": 5.20703125,
    "adjusted_score": 0.4140625,
    "task_macro_score": -0.044225324575324285,
    "adjusted_task_macro_score": -0.044225324575324285,
    "task_categorized_scores": {
      "Coding & Debugging": -1.115384615384615,
      "Creative Tasks": 1.376344086021506,
      "Math & Data Analysis": -1.046153846153846,
      "Planning & Reasoning": 0.621468926553673,
      "Information/Advice seeking": 1.0204081632653068
    },
    "total": 256,
    "avg_len": 1948.578125
  },
  "adv_2_690": {
    "model": "adv_2_690",
    "score": 5.203921568627451,
    "adjusted_score": 0.4078431372549023,
    "task_macro_score": -0.17745519628535975,
    "adjusted_task_macro_score": -0.17745519628535975,
    "task_categorized_scores": {
      "Creative Tasks": 1.9120879120879124,
      "Math & Data Analysis": -1.7230769230769223,
      "Planning & Reasoning": 0.4886363636363633,
      "Information/Advice seeking": 0.6530612244897966,
      "Coding & Debugging": -0.9411764705882355
    },
    "total": 255,
    "avg_len": 1851.9448818897638
  },
  "adv_2_165": {
    "model": "adv_2_165",
    "score": 5.19921875,
    "adjusted_score": 0.3984375,
    "task_macro_score": -0.16574066038870758,
    "adjusted_task_macro_score": -0.16574066038870758,
    "task_categorized_scores": {
      "Coding & Debugging": -1.3076923076923084,
      "Creative Tasks": 2.021505376344086,
      "Math & Data Analysis": -1.5076923076923077,
      "Planning & Reasoning": 0.7570621468926557,
      "Information/Advice seeking": 0.5306122448979593
    },
    "total": 256,
    "avg_len": 2195.203125
  },
  "adv_2_760": {
    "model": "adv_2_760",
    "score": 5.19921875,
    "adjusted_score": 0.3984375,
    "task_macro_score": -0.26642367907575243,
    "adjusted_task_macro_score": -0.26642367907575243,
    "task_categorized_scores": {
      "Creative Tasks": 2.1304347826086953,
      "Math & Data Analysis": -1.661538461538461,
      "Planning & Reasoning": 0.5310734463276834,
      "Information/Advice seeking": 0.5102040816326525,
      "Coding & Debugging": -1.3725490196078436
    },
    "total": 256,
    "avg_len": 1735.3294117647058
  },
  "adv_1_15": {
    "model": "adv_1_15",
    "score": 5.19140625,
    "adjusted_score": 0.3828125,
    "task_macro_score": -0.039825002457115934,
    "adjusted_task_macro_score": -0.039825002457115934,
    "task_categorized_scores": {
      "Coding & Debugging": -1.0,
      "Creative Tasks": 1.161290322580646,
      "Math & Data Analysis": -0.9230769230769234,
      "Planning & Reasoning": 0.48587570621468856,
      "Information/Advice seeking": 1.0612244897959187
    },
    "total": 256,
    "avg_len": 4155.234375
  },
  "adv_2_540": {
    "model": "adv_2_540",
    "score": 5.19140625,
    "adjusted_score": 0.3828125,
    "task_macro_score": -0.1983110303364319,
    "adjusted_task_macro_score": -0.1983110303364319,
    "task_categorized_scores": {
      "Creative Tasks": 1.7173913043478262,
      "Math & Data Analysis": -1.3230769230769237,
      "Planning & Reasoning": 0.5423728813559325,
      "Information/Advice seeking": 0.8775510204081627,
      "Coding & Debugging": -1.450980392156863
    },
    "total": 256,
    "avg_len": 1849.764705882353
  },
  "adv_1_90": {
    "model": "adv_1_90",
    "score": 5.17578125,
    "adjusted_score": 0.3515625,
    "task_macro_score": -0.10595853484732291,
    "adjusted_task_macro_score": -0.10595853484732291,
    "task_categorized_scores": {
      "Coding & Debugging": -1.1538461538461533,
      "Creative Tasks": 1.268817204301076,
      "Math & Data Analysis": -1.1692307692307686,
      "Planning & Reasoning": 0.6892655367231644,
      "Information/Advice seeking": 0.816326530612244
    },
    "total": 256,
    "avg_len": 2511.82421875
  },
  "adv_1_690": {
    "model": "adv_1_690",
    "score": 5.15625,
    "adjusted_score": 0.3125,
    "task_macro_score": -0.15519405784863097,
    "adjusted_task_macro_score": -0.15519405784863097,
    "task_categorized_scores": {
      "Creative Tasks": 1.4130434782608692,
      "Math & Data Analysis": -1.0769230769230766,
      "Planning & Reasoning": 0.5310734463276834,
      "Information/Advice seeking": 0.9591836734693882,
      "Coding & Debugging": -1.4000000000000004
    },
    "total": 256,
    "avg_len": 1741.9251968503936
  },
  "adv_1_760": {
    "model": "adv_1_760",
    "score": 5.14453125,
    "adjusted_score": 0.2890625,
    "task_macro_score": -0.18030022865434903,
    "adjusted_task_macro_score": -0.18030022865434903,
    "task_categorized_scores": {
      "Creative Tasks": 1.5434782608695645,
      "Math & Data Analysis": -1.138461538461538,
      "Planning & Reasoning": 0.5649717514124291,
      "Information/Advice seeking": 0.9387755102040813,
      "Coding & Debugging": -1.5199999999999996
    },
    "total": 256,
    "avg_len": 1772.0118110236222
  },
  "adv_1_240": {
    "model": "adv_1_240",
    "score": 5.12890625,
    "adjusted_score": 0.2578125,
    "task_macro_score": -0.20495833608923242,
    "adjusted_task_macro_score": -0.20495833608923242,
    "task_categorized_scores": {
      "Creative Tasks": 1.3478260869565215,
      "Math & Data Analysis": -1.138461538461538,
      "Planning & Reasoning": 0.6666666666666661,
      "Information/Advice seeking": 0.6938775510204085,
      "Coding & Debugging": -1.4901960784313726
    },
    "total": 256,
    "avg_len": 1858.556862745098
  },
  "adv_2_90": {
    "model": "adv_2_90",
    "score": 5.11328125,
    "adjusted_score": 0.2265625,
    "task_macro_score": -0.2363566104983618,
    "adjusted_task_macro_score": -0.2363566104983618,
    "task_categorized_scores": {
      "Coding & Debugging": -1.0769230769230766,
      "Creative Tasks": 1.268817204301076,
      "Math & Data Analysis": -1.3538461538461544,
      "Planning & Reasoning": 0.4971751412429377,
      "Information/Advice seeking": 0.4285714285714288
    },
    "total": 256,
    "avg_len": 2588.91796875
  },
  "gemma-2b-it": {
    "model": "gemma-2b-it",
    "score": 4.737512242899118,
    "adjusted_score": -0.5249755142017634,
    "task_macro_score": -0.9691930072258819,
    "adjusted_task_macro_score": -0.9691930072258819,
    "task_categorized_scores": {
      "Planning & Reasoning": -0.5795795795795797,
      "Information/Advice seeking": -0.2133995037220835,
      "Coding & Debugging": -1.7725118483412317,
      "Creative Tasks": 0.7220779220779221,
      "Math & Data Analysis": -1.8645418326693228
    },
    "total": 1021,
    "avg_len": 1590.0833333333333
  }
}