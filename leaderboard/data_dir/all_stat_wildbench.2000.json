{
  "gpt-4o-2024-05-13": {
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "57.5",
    "AE2.0": "51.3",
    "Arena Elo (hard-en) - 2024-07-16": 1280,
    "Arena Elo (hard-en) - latest": 1280,
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 59.12144702842377,
    "WB_score.Planning & Reasoning": 60.20958083832337,
    "WB_score.Math & Data Analysis": 57.29083665338646,
    "WB_score.Information/Advice seeking": 58.61386138613861,
    "WB_score.Coding & Debugging": 60.473933649289116,
    "WB_score": 58.80742913000978,
    "WB_score.task_macro": 59.298178803519555,
    "Length": 3723.516129032258,
    "Rank_ScoreMacro": 2,
    "Rank_TaskMacroReward.K": 1,
    "Rank_Avg": 1.5,
    "RewardScore_Avg": 29.649089401759777,
    "WB_Elo": 1252.602416320019
  },
  "gpt-4-turbo-2024-04-09": {
    "Arena-Hard v0.1": "82.6",
    "AE2.0 LC": "55",
    "AE2.0": "46.1",
    "Arena Elo (hard-en) - 2024-07-16": 1247,
    "Arena Elo (hard-en) - latest": 1247,
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 58.65633074935401,
    "WB_score.Planning & Reasoning": 56.203288490284,
    "WB_score.Math & Data Analysis": 50.99601593625499,
    "WB_score.Information/Advice seeking": 57.178217821782184,
    "WB_score.Coding & Debugging": 55.071090047393355,
    "WB_score": 56.089931573802545,
    "WB_score.task_macro": 55.22122481039269,
    "Length": 3093.1700879765394,
    "Rank_ScoreMacro": 6,
    "Rank_TaskMacroReward.K": 2,
    "Rank_Avg": 4.0,
    "RewardScore_Avg": 27.610612405196346,
    "WB_Elo": 1230.2995360289044
  },
  "gpt-4-0125-preview": {
    "Arena-Hard v0.1": "78",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "Arena Elo (hard-en) - 2024-07-16": 1237,
    "Arena Elo (hard-en) - latest": 1237,
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 57.571059431524546,
    "WB_score.Planning & Reasoning": 53.45291479820627,
    "WB_score.Math & Data Analysis": 45.79365079365079,
    "WB_score.Information/Advice seeking": 54.35643564356436,
    "WB_score.Coding & Debugging": 52.924528301886795,
    "WB_score": 53.28125,
    "WB_score.task_macro": 52.27753918256898,
    "Length": 3335.638671875,
    "Rank_ScoreMacro": 12,
    "Rank_TaskMacroReward.K": 3,
    "Rank_Avg": 7.5,
    "RewardScore_Avg": 26.13876959128449,
    "WB_Elo": 1214.4295437978471
  },
  "claude-3-opus-20240229": {
    "Arena-Hard v0.1": "60.4",
    "AE2.0 LC": "40.5",
    "AE2.0": "29.1",
    "Arena Elo (hard-en) - 2024-07-16": 1230,
    "Arena Elo (hard-en) - latest": 1230,
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 53.0232558139535,
    "WB_score.Planning & Reasoning": 52.526158445440956,
    "WB_score.Math & Data Analysis": 46.74603174603174,
    "WB_score.Information/Advice seeking": 53.46534653465346,
    "WB_score.Coding & Debugging": 53.301886792452834,
    "WB_score": 52.109375,
    "WB_score.task_macro": 51.714047600287536,
    "Length": 2685.9794921875,
    "Rank_ScoreMacro": 13,
    "Rank_TaskMacroReward.K": 4,
    "Rank_Avg": 8.5,
    "RewardScore_Avg": 25.857023800143768,
    "WB_Elo": 1215.2585698527512
  },
  "Meta-Llama-3-70B-Instruct": {
    "Arena-Hard v0.1": "41.1",
    "AE2.0 LC": "34.4",
    "AE2.0": "33.2",
    "Arena Elo (hard-en) - 2024-07-16": 1212,
    "Arena Elo (hard-en) - latest": 1212,
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 54.30051813471502,
    "WB_score.Planning & Reasoning": 50.07473841554558,
    "WB_score.Math & Data Analysis": 42.063492063492056,
    "WB_score.Information/Advice seeking": 52.27722772277227,
    "WB_score.Coding & Debugging": 44.71698113207546,
    "WB_score": 49.579667644183765,
    "WB_score.task_macro": 47.770804496306326,
    "Length": 3046.6383186705766,
    "Rank_ScoreMacro": 18,
    "Rank_TaskMacroReward.K": 5,
    "Rank_Avg": 11.5,
    "RewardScore_Avg": 23.885402248153163,
    "WB_Elo": 1201.2894671924937
  },
  "Qwen1.5-72B-Chat-greedy": {
    "Arena-Hard v0.1": "36.1",
    "AE2.0 LC": "36.6",
    "AE2.0": "26.5",
    "Arena Elo (hard-en) - 2024-07-16": 1142,
    "Arena Elo (hard-en) - latest": 1142,
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 50.362694300518136,
    "WB_score.Planning & Reasoning": 43.45345345345345,
    "WB_score.Math & Data Analysis": 29.800796812748995,
    "WB_score.Information/Advice seeking": 48.21782178217822,
    "WB_score.Coding & Debugging": 35.35545023696683,
    "WB_score": 43.46718903036239,
    "WB_score.task_macro": 39.927713665824655,
    "Length": 2392.364348677767,
    "Rank_ScoreMacro": 28,
    "Rank_TaskMacroReward.K": 6,
    "Rank_Avg": 17.0,
    "RewardScore_Avg": 19.963856832912327,
    "WB_Elo": 1150.9914399857998
  },
  "claude-3-sonnet-20240229": {
    "Arena-Hard v0.1": "46.8",
    "AE2.0 LC": "34.9",
    "AE2.0": "25.6",
    "Arena Elo (hard-en) - 2024-07-16": 1188,
    "Arena Elo (hard-en) - latest": 1188,
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 46.304909560723516,
    "WB_score.Planning & Reasoning": 47.425149700598794,
    "WB_score.Math & Data Analysis": 40.63745019920319,
    "WB_score.Information/Advice seeking": 47.128712871287135,
    "WB_score.Coding & Debugging": 46.09523809523809,
    "WB_score": 45.24461839530332,
    "WB_score.task_macro": 45.48145776375293,
    "Length": 2670.243639921722,
    "Rank_ScoreMacro": 24,
    "Rank_TaskMacroReward.K": 7,
    "Rank_Avg": 15.5,
    "RewardScore_Avg": 22.740728881876464,
    "WB_Elo": 1179.4624921655945
  },
  "mistral-large-2402": {
    "Arena-Hard v0.1": "37.7",
    "AE2.0 LC": "32.7",
    "AE2.0": "21.4",
    "Arena Elo (hard-en) - 2024-07-16": 1158,
    "Arena Elo (hard-en) - latest": 1158,
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 49.66408268733851,
    "WB_score.Planning & Reasoning": 41.79910044977511,
    "WB_score.Math & Data Analysis": 30.879999999999992,
    "WB_score.Information/Advice seeking": 46.13861386138615,
    "WB_score.Coding & Debugging": 33.74407582938389,
    "WB_score": 42.28739002932551,
    "WB_score.task_macro": 38.89367833445423,
    "Length": 2514.9814090019568,
    "Rank_ScoreMacro": 31,
    "Rank_TaskMacroReward.K": 8,
    "Rank_Avg": 19.5,
    "RewardScore_Avg": 19.446839167227115,
    "WB_Elo": 1161.2167637794778
  },
  "claude-3-haiku-20240307": {
    "Arena-Hard v0.1": "41.5",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "Arena Elo (hard-en) - 2024-07-16": 1170,
    "Arena Elo (hard-en) - latest": 1170,
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 42.94573643410853,
    "WB_score.Planning & Reasoning": 41.28550074738415,
    "WB_score.Math & Data Analysis": 31.428571428571423,
    "WB_score.Information/Advice seeking": 45.346534653465355,
    "WB_score.Coding & Debugging": 36.9811320754717,
    "WB_score": 40.25390625,
    "WB_score.task_macro": 38.893606666167265,
    "Length": 2601.029296875,
    "Rank_ScoreMacro": 32,
    "Rank_TaskMacroReward.K": 9,
    "Rank_Avg": 20.5,
    "RewardScore_Avg": 19.446803333083633,
    "WB_Elo": 1165.1759406842987
  },
  "dbrx-instruct@together": {
    "Arena-Hard v0.1": "23.9",
    "AE2.0 LC": "25.4",
    "AE2.0": "18.4",
    "Arena Elo (hard-en) - latest": "-",
    "Arena Elo (hard-en) - 2024-07-16": "-",
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 42.32558139534884,
    "WB_score.Planning & Reasoning": 36.227544910179645,
    "WB_score.Math & Data Analysis": 24.523809523809526,
    "WB_score.Information/Advice seeking": 41.089108910891085,
    "WB_score.Coding & Debugging": 26.445497630331758,
    "WB_score": 35.5425219941349,
    "WB_score.task_macro": 32.598891595850844,
    "Length": 2576.5190615835777,
    "Rank_ScoreMacro": 39,
    "Rank_TaskMacroReward.K": 10,
    "Rank_Avg": 24.5,
    "RewardScore_Avg": 16.299445797925422,
    "WB_Elo": 1120.146362044812
  },
  "Mixtral-8x7B-Instruct-v0.1": {
    "Arena-Hard v0.1": "23.4",
    "AE2.0 LC": "23.7",
    "AE2.0": "18.3",
    "Arena Elo (hard-en) - 2024-07-16": 1114,
    "Arena Elo (hard-en) - latest": 1114,
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 42.753246753246756,
    "WB_score.Planning & Reasoning": 34.586466165413526,
    "WB_score.Math & Data Analysis": 22.142857142857135,
    "WB_score.Information/Advice seeking": 41.935483870967744,
    "WB_score.Coding & Debugging": 25.023696682464447,
    "WB_score": 35.0293542074364,
    "WB_score.task_macro": 31.47027304895869,
    "Length": 2653.5813725490198,
    "Rank_ScoreMacro": 41,
    "Rank_TaskMacroReward.K": 11,
    "Rank_Avg": 26.0,
    "RewardScore_Avg": 15.735136524479344,
    "WB_Elo": 1122.854614406154
  },
  "Starling-LM-7B-beta": {
    "Arena-Hard v0.1": "23",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "Arena Elo (hard-en) - 2024-07-16": 1113,
    "Arena Elo (hard-en) - latest": 1113,
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 43.79220779220779,
    "WB_score.Planning & Reasoning": 34.050822122571006,
    "WB_score.Math & Data Analysis": 16.984126984126977,
    "WB_score.Information/Advice seeking": 41.88118811881188,
    "WB_score.Coding & Debugging": 24.36018957345972,
    "WB_score": 34.17399804496579,
    "WB_score.task_macro": 30.169449808290146,
    "Length": 2797.807240704501,
    "Rank_ScoreMacro": 44,
    "Rank_TaskMacroReward.K": 12,
    "Rank_Avg": 28.0,
    "RewardScore_Avg": 15.084724904145073,
    "WB_Elo": 1121.669986490806
  },
  "command-r": {
    "Arena-Hard v0.1": "17",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "Arena Elo (hard-en) - 2024-07-16": 1106,
    "Arena Elo (hard-en) - latest": 1106,
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 47.44186046511628,
    "WB_score.Planning & Reasoning": 34.61883408071749,
    "WB_score.Math & Data Analysis": 16.031746031746028,
    "WB_score.Information/Advice seeking": 44.10891089108912,
    "WB_score.Coding & Debugging": 19.33962264150944,
    "WB_score": 35.05859375,
    "WB_score.task_macro": 29.533143228506248,
    "Length": 2919.423828125,
    "Rank_ScoreMacro": 47,
    "Rank_TaskMacroReward.K": 13,
    "Rank_Avg": 30.0,
    "RewardScore_Avg": 14.766571614253124,
    "WB_Elo": 1114.8058709020904
  },
  "command-r-plus": {
    "Arena-Hard v0.1": "33.1",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "Arena Elo (hard-en) - 2024-07-16": 1155,
    "Arena Elo (hard-en) - latest": 1155,
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 52.55813953488372,
    "WB_score.Planning & Reasoning": 41.949025487256364,
    "WB_score.Math & Data Analysis": 23.492063492063497,
    "WB_score.Information/Advice seeking": 49.15841584158416,
    "WB_score.Coding & Debugging": 28.436018957345972,
    "WB_score": 41.565557729941304,
    "WB_score.task_macro": 36.76236856767293,
    "Length": 3293.812133072407,
    "Rank_ScoreMacro": 36,
    "Rank_TaskMacroReward.K": 14,
    "Rank_Avg": 25.0,
    "RewardScore_Avg": 18.381184283836465,
    "WB_Elo": 1149.4741617454304
  },
  "Meta-Llama-3-8B-Instruct": {
    "Arena-Hard v0.1": "20.6",
    "AE2.0 LC": "22.9",
    "AE2.0": "22.6",
    "Arena Elo (hard-en) - 2024-07-16": 1144,
    "Arena Elo (hard-en) - latest": 1144,
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 43.56589147286822,
    "WB_score.Planning & Reasoning": 34.401197604790426,
    "WB_score.Math & Data Analysis": 16.972111553784863,
    "WB_score.Information/Advice seeking": 39.30693069306932,
    "WB_score.Coding & Debugging": 21.9811320754717,
    "WB_score": 33.176930596285445,
    "WB_score.task_macro": 29.20277208638918,
    "Length": 2975.1876832844573,
    "Rank_ScoreMacro": 48,
    "Rank_TaskMacroReward.K": 15,
    "Rank_Avg": 31.5,
    "RewardScore_Avg": 14.60138604319459,
    "WB_Elo": 1137.5706597255137
  },
  "tulu-2-dpo-70b": {
    "Arena-Hard v0.1": "15",
    "AE2.0 LC": "21.2",
    "AE2.0": "16",
    "Arena Elo (hard-en) - 2024-07-16": 1101,
    "Arena Elo (hard-en) - latest": 1101,
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 42.7012987012987,
    "WB_score.Planning & Reasoning": 32.30538922155688,
    "WB_score.Math & Data Analysis": 14.841269841269842,
    "WB_score.Information/Advice seeking": 40.69306930693068,
    "WB_score.Coding & Debugging": 20.663507109004744,
    "WB_score": 32.82502443792767,
    "WB_score.task_macro": 27.983756123225106,
    "Length": 2908.0714285714284,
    "Rank_ScoreMacro": 49,
    "Rank_TaskMacroReward.K": 16,
    "Rank_Avg": 32.5,
    "RewardScore_Avg": 13.991878061612553,
    "WB_Elo": 1113.322454471141
  },
  "Nous-Hermes-2-Mixtral-8x7B-DPO": {
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "Arena Elo (hard-en) - 2024-07-16": 1046,
    "Arena Elo (hard-en) - latest": 1046,
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 37.92207792207792,
    "WB_score.Planning & Reasoning": 34.24287856071963,
    "WB_score.Math & Data Analysis": 21.752988047808763,
    "WB_score.Information/Advice seeking": 39.75247524752476,
    "WB_score.Coding & Debugging": 26.037735849056602,
    "WB_score": 33.22233104799217,
    "WB_score.task_macro": 30.711400306676122,
    "Length": 2874.541625857003,
    "Rank_ScoreMacro": 42,
    "Rank_TaskMacroReward.K": 17,
    "Rank_Avg": 29.5,
    "RewardScore_Avg": 15.355700153338061,
    "WB_Elo": 1080.628823996117
  },
  "Mistral-7B-Instruct-v0.2": {
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "17.1",
    "AE2.0": "14.7",
    "Arena Elo (hard-en) - 2024-07-16": 1072,
    "Arena Elo (hard-en) - latest": 1072,
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 42.072538860103634,
    "WB_score.Planning & Reasoning": 30.059880239520957,
    "WB_score.Math & Data Analysis": 10.079365079365079,
    "WB_score.Information/Advice seeking": 40.099255583126556,
    "WB_score.Coding & Debugging": 18.396226415094343,
    "WB_score": 30.694037145650057,
    "WB_score.task_macro": 25.633728318953878,
    "Length": 2832.3440860215055,
    "Rank_ScoreMacro": 52,
    "Rank_TaskMacroReward.K": 18,
    "Rank_Avg": 35.0,
    "RewardScore_Avg": 12.816864159476939,
    "WB_Elo": 1097.0804963445955
  },
  "gpt-3.5-turbo-0125": {
    "Arena-Hard v0.1": "23.3",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "Arena Elo (hard-en) - 2024-07-16": 1107,
    "Arena Elo (hard-en) - latest": 1107,
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 37.41602067183463,
    "WB_score.Planning & Reasoning": 33.3931240657698,
    "WB_score.Math & Data Analysis": 21.58730158730158,
    "WB_score.Information/Advice seeking": 36.485148514851474,
    "WB_score.Coding & Debugging": 26.54028436018958,
    "WB_score": 32.27761485826002,
    "WB_score.task_macro": 30.01598607195931,
    "Length": 1844.13880742913,
    "Rank_ScoreMacro": 45,
    "Rank_TaskMacroReward.K": 19,
    "Rank_Avg": 32.0,
    "RewardScore_Avg": 15.007993035979656,
    "WB_Elo": 1129.1105197245229
  },
  "Qwen1.5-7B-Chat@together": {
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "14.7",
    "AE2.0": "11.8",
    "Arena Elo (hard-en) - latest": "-",
    "Arena Elo (hard-en) - 2024-07-16": "-",
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 38.29457364341085,
    "WB_score.Planning & Reasoning": 28.878923766816147,
    "WB_score.Math & Data Analysis": 11.904761904761898,
    "WB_score.Information/Advice seeking": 34.00990099009901,
    "WB_score.Coding & Debugging": 14.88151658767773,
    "WB_score": 27.370478983382203,
    "WB_score.task_macro": 23.42316313940188,
    "Length": 2519.4203323558163,
    "Rank_ScoreMacro": 55,
    "Rank_TaskMacroReward.K": 20,
    "Rank_Avg": 37.5,
    "RewardScore_Avg": 11.71158156970094,
    "WB_Elo": 1083.0428881366172
  },
  "Llama-2-70b-chat-hf": {
    "Arena-Hard v0.1": "11.6",
    "AE2.0 LC": "14.7",
    "AE2.0": "13.9",
    "Arena Elo (hard-en) - 2024-07-16": 1071,
    "Arena Elo (hard-en) - latest": 1071,
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 40.0,
    "WB_score.Planning & Reasoning": 26.846846846846848,
    "WB_score.Math & Data Analysis": 4.176706827309236,
    "WB_score.Information/Advice seeking": 38.30845771144279,
    "WB_score.Coding & Debugging": 9.333333333333336,
    "WB_score": 26.9140625,
    "WB_score.task_macro": 20.65963691286665,
    "Length": 3138.3179587831205,
    "Rank_ScoreMacro": 59,
    "Rank_TaskMacroReward.K": 21,
    "Rank_Avg": 40.0,
    "RewardScore_Avg": 10.329818456433324,
    "WB_Elo": 1086.5078065544553
  },
  "Llama-2-7b-chat-hf": {
    "Arena-Hard v0.1": "4.6",
    "AE2.0 LC": "5.4",
    "AE2.0": "5",
    "Arena Elo (hard-en) - 2024-07-16": 1012,
    "Arena Elo (hard-en) - latest": 1012,
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 29.76623376623376,
    "WB_score.Planning & Reasoning": 15.428571428571427,
    "WB_score.Math & Data Analysis": -7.177419354838701,
    "WB_score.Information/Advice seeking": 27.66169154228855,
    "WB_score.Coding & Debugging": -6.794258373205739,
    "WB_score": 15.225048923679054,
    "WB_score.task_macro": 8.262075264042466,
    "Length": 2985.1052114060963,
    "Rank_ScoreMacro": 60,
    "Rank_TaskMacroReward.K": 22,
    "Rank_Avg": 41.0,
    "RewardScore_Avg": 4.131037632021233,
    "WB_Elo": 1032.7228919369597
  },
  "gemma-7b-it": {
    "Arena-Hard v0.1": "7.5",
    "AE2.0 LC": "10.4",
    "AE2.0": "6.9",
    "Arena Elo (hard-en) - 2024-07-16": 1046,
    "Arena Elo (hard-en) - latest": 1046,
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 21.19170984455959,
    "WB_score.Planning & Reasoning": 10.164424514200299,
    "WB_score.Math & Data Analysis": -3.6507936507936556,
    "WB_score.Information/Advice seeking": 12.72277227722773,
    "WB_score.Coding & Debugging": 1.8009478672985857,
    "WB_score": 10.17578125,
    "WB_score.task_macro": 6.61975914869064,
    "Length": 1726.3440860215053,
    "Rank_ScoreMacro": 61,
    "Rank_TaskMacroReward.K": 23,
    "Rank_Avg": 42.0,
    "RewardScore_Avg": 3.30987957434532,
    "WB_Elo": 1061.717323608561
  },
  "gemma-2b-it": {
    "Arena-Hard v0.1": "3",
    "AE2.0 LC": "5.4",
    "AE2.0": "3.4",
    "Arena Elo (hard-en) - 2024-07-16": 977,
    "Arena Elo (hard-en) - latest": 977,
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 7.220779220779221,
    "WB_score.Planning & Reasoning": -5.795795795795797,
    "WB_score.Math & Data Analysis": -18.64541832669323,
    "WB_score.Information/Advice seeking": -2.133995037220835,
    "WB_score.Coding & Debugging": -17.725118483412317,
    "WB_score": -5.249755142017634,
    "WB_score.task_macro": -9.691930072258819,
    "Length": 1590.0833333333333,
    "Rank_ScoreMacro": 95,
    "Rank_TaskMacroReward.K": 24,
    "Rank_Avg": 59.5,
    "RewardScore_Avg": -4.845965036129409,
    "WB_Elo": 1009.8213525831305
  },
  "Llama-3-Instruct-8B-SimPO": {
    "Arena-Hard v0.1": "33.8",
    "AE2.0 LC": "44.7",
    "AE2.0": "40.5",
    "Arena Elo (hard-en) - latest": "-",
    "Arena Elo (hard-en) - 2024-07-16": "-",
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 50.64599483204134,
    "WB_score.Planning & Reasoning": 40.86696562032884,
    "WB_score.Math & Data Analysis": 23.984063745019917,
    "WB_score.Information/Advice seeking": 47.871287128712865,
    "WB_score.Coding & Debugging": 31.753554502369674,
    "WB_score": 41.17302052785924,
    "WB_score.task_macro": 37.049721402304925,
    "Length": 2541.9257086999023,
    "Rank_ScoreMacro": 35,
    "Rank_TaskMacroReward.K": 25,
    "Rank_Avg": 30.0,
    "RewardScore_Avg": 18.524860701152463,
    "WB_Elo": 1149.4296944507707
  },
  "SELM-Zephyr-7B-iter-3": {
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "24.00",
    "AE2.0": "-",
    "Arena Elo (hard-en) - latest": "-",
    "Arena Elo (hard-en) - 2024-07-16": "-",
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 44.70284237726098,
    "WB_score.Planning & Reasoning": 31.58682634730539,
    "WB_score.Math & Data Analysis": 12.669322709163353,
    "WB_score.Information/Advice seeking": 40.99009900990099,
    "WB_score.Coding & Debugging": 11.037735849056602,
    "WB_score": 31.5234375,
    "WB_score.task_macro": 25.061899136983598,
    "Length": 2823.7800586510266,
    "Rank_ScoreMacro": 53,
    "Rank_TaskMacroReward.K": 26,
    "Rank_Avg": 39.5,
    "RewardScore_Avg": 12.530949568491799,
    "WB_Elo": 1122.742882339819
  },
  "Qwen2-72B-Instruct": {
    "Arena-Hard v0.1": "48.1",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "Arena Elo (hard-en) - 2024-07-16": 1182,
    "Arena Elo (hard-en) - latest": 1182,
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 49.92248062015504,
    "WB_score.Planning & Reasoning": 46.84603886397609,
    "WB_score.Math & Data Analysis": 40.95238095238095,
    "WB_score.Information/Advice seeking": 49.50495049504951,
    "WB_score.Coding & Debugging": 39.81132075471699,
    "WB_score": 46.40625,
    "WB_score.task_macro": 44.497691296234095,
    "Length": 2856.4482421875,
    "Rank_ScoreMacro": 25,
    "Rank_TaskMacroReward.K": 27,
    "Rank_Avg": 26.0,
    "RewardScore_Avg": 22.248845648117047,
    "WB_Elo": 1178.4774268733538
  },
  "Hermes-2-Theta-Llama-3-8B": {
    "Arena Elo (hard-en) - latest": "-",
    "Arena Elo (hard-en) - 2024-07-16": "-",
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 39.79328165374676,
    "WB_score.Planning & Reasoning": 33.65269461077844,
    "WB_score.Math & Data Analysis": 18.725099601593627,
    "WB_score.Information/Advice seeking": 41.584158415841586,
    "WB_score.Coding & Debugging": 23.113207547169807,
    "WB_score": 32.9423264907136,
    "WB_score.task_macro": 29.635207776375477,
    "Length": 2742.169110459433,
    "Rank_ScoreMacro": 46,
    "Rank_TaskMacroReward.K": 28,
    "Rank_Avg": 37.0,
    "RewardScore_Avg": 14.817603888187739,
    "WB_Elo": 1125.226487228437
  },
  "yi-large": {
    "Arena-Hard v0.1": "63.7",
    "AE2.0 LC": "51.9",
    "AE2.0": "57.5",
    "Arena Elo (hard-en) - 2024-07-16": 1198,
    "Arena Elo (hard-en) - latest": 1198,
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 51.80156657963445,
    "WB_score.Planning & Reasoning": 51.33834586466165,
    "WB_score.Math & Data Analysis": 44.46215139442231,
    "WB_score.Information/Advice seeking": 50.96774193548388,
    "WB_score.Coding & Debugging": 47.71428571428572,
    "WB_score": 48.93450635386118,
    "WB_score.task_macro": 48.92726960200772,
    "Length": 3095.335952848723,
    "Rank_ScoreMacro": 14,
    "Rank_TaskMacroReward.K": 29,
    "Rank_Avg": 21.5,
    "RewardScore_Avg": 24.46363480100386,
    "WB_Elo": 1186.313679282338
  },
  "Yi-1.5-34B-Chat": {
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "Arena Elo (hard-en) - 2024-07-16": 1160,
    "Arena Elo (hard-en) - latest": 1160,
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 53.523316062176164,
    "WB_score.Planning & Reasoning": 48.108108108108105,
    "WB_score.Math & Data Analysis": 39.43775100401606,
    "WB_score.Information/Advice seeking": 50.29702970297029,
    "WB_score.Coding & Debugging": 42.08530805687204,
    "WB_score": 47.350928641251215,
    "WB_score.task_macro": 45.613463477590955,
    "Length": 3523.557843137255,
    "Rank_ScoreMacro": 23,
    "Rank_TaskMacroReward.K": 30,
    "Rank_Avg": 26.5,
    "RewardScore_Avg": 22.806731738795477,
    "WB_Elo": 1158.5375815166344
  },
  "reka-flash-20240226": {
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "Arena Elo (hard-en) - 2024-07-16": 1127,
    "Arena Elo (hard-en) - latest": 1127,
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 42.44155844155845,
    "WB_score.Planning & Reasoning": 35.01501501501501,
    "WB_score.Math & Data Analysis": 20.48,
    "WB_score.Information/Advice seeking": 41.53465346534654,
    "WB_score.Coding & Debugging": 22.085308056872037,
    "WB_score": 34.60410557184751,
    "WB_score.task_macro": 30.363615402031144,
    "Length": 2103.0098039215686,
    "Rank_ScoreMacro": 43,
    "Rank_TaskMacroReward.K": 31,
    "Rank_Avg": 37.0,
    "RewardScore_Avg": 15.181807701015572,
    "WB_Elo": 1134.503909557202
  },
  "gemini-1.5-pro": {
    "Arena-Hard v0.1": "72.0",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "Arena Elo (hard-en) - 2024-07-16": 1245,
    "Arena Elo (hard-en) - latest": 1245,
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 55.124653739612185,
    "WB_score.Planning & Reasoning": 53.73271889400922,
    "WB_score.Math & Data Analysis": 48.59437751004016,
    "WB_score.Information/Advice seeking": 52.22506393861893,
    "WB_score.Coding & Debugging": 55.223880597014926,
    "WB_score": 47.3828125,
    "WB_score.task_macro": 52.95184246265066,
    "Length": 3247.9673135852913,
    "Rank_ScoreMacro": 11,
    "Rank_TaskMacroReward.K": 32,
    "Rank_Avg": 21.5,
    "RewardScore_Avg": 26.47592123132533,
    "WB_Elo": 1223.1507519158076
  },
  "gemini-1.5-flash": {
    "Arena-Hard v0.1": "49.6",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "Arena Elo (hard-en) - 2024-07-16": 1212,
    "Arena Elo (hard-en) - latest": 1212,
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 51.65745856353592,
    "WB_score.Planning & Reasoning": 50.78582434514638,
    "WB_score.Math & Data Analysis": 45.322580645161295,
    "WB_score.Information/Advice seeking": 48.66666666666667,
    "WB_score.Coding & Debugging": 48.72549019607844,
    "WB_score": 44.14872798434443,
    "WB_score.task_macro": 48.85062170599163,
    "Length": 3654.3993871297243,
    "Rank_ScoreMacro": 15,
    "Rank_TaskMacroReward.K": 33,
    "Rank_Avg": 24.0,
    "RewardScore_Avg": 24.425310852995814,
    "WB_Elo": 1197.205124627846
  },
  "reka-core-20240501": {
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "Arena Elo (hard-en) - 2024-07-16": 1175,
    "Arena Elo (hard-en) - latest": 1175,
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 55.4874651810585,
    "WB_score.Planning & Reasoning": 48.00632911392405,
    "WB_score.Math & Data Analysis": 40.34188034188034,
    "WB_score.Information/Advice seeking": 52.254641909814325,
    "WB_score.Coding & Debugging": 40.60301507537689,
    "WB_score": 41.03515625,
    "WB_score.task_macro": 45.90279465292558,
    "Length": 2592.589397089397,
    "Rank_ScoreMacro": 21,
    "Rank_TaskMacroReward.K": 34,
    "Rank_Avg": 27.5,
    "RewardScore_Avg": 22.95139732646279,
    "WB_Elo": 1175.8117303594756
  },
  "yi-large-preview": {
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "Arena Elo (hard-en) - 2024-07-16": 1229,
    "Arena Elo (hard-en) - latest": 1229,
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 57.64397905759162,
    "WB_score.Planning & Reasoning": 56.606606606606604,
    "WB_score.Math & Data Analysis": 51.92,
    "WB_score.Information/Advice seeking": 57.72277227722773,
    "WB_score.Coding & Debugging": 54.28571428571429,
    "WB_score": 54.83870967741936,
    "WB_score.task_macro": 55.294625232024785,
    "Length": 3512.678149606299,
    "Rank_ScoreMacro": 5,
    "Rank_TaskMacroReward.K": 35,
    "Rank_Avg": 20.0,
    "RewardScore_Avg": 27.647312616012393,
    "WB_Elo": 1206.1557477971662
  },
  "nemotron-4-340b-instruct": {
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "Arena Elo (hard-en) - 2024-07-16": 1185,
    "Arena Elo (hard-en) - latest": 1185,
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 53.3160621761658,
    "WB_score.Planning & Reasoning": 49.12912912912914,
    "WB_score.Math & Data Analysis": 40.80321285140562,
    "WB_score.Information/Advice seeking": 53.00248138957816,
    "WB_score.Coding & Debugging": 46.25592417061611,
    "WB_score": 48.84765625,
    "WB_score.task_macro": 47.67250981186394,
    "Length": 2754.0098039215686,
    "Rank_ScoreMacro": 19,
    "Rank_TaskMacroReward.K": 36,
    "Rank_Avg": 27.5,
    "RewardScore_Avg": 23.83625490593197,
    "WB_Elo": 1185.2855137057322
  },
  "claude-3-5-sonnet-20240620": {
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "Arena Elo (hard-en) - 2024-07-16": 1271,
    "Arena Elo (hard-en) - latest": 1271,
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 55.60723514211887,
    "WB_score.Planning & Reasoning": 55.635276532137524,
    "WB_score.Math & Data Analysis": 50.15873015873016,
    "WB_score.Information/Advice seeking": 55.54455445544555,
    "WB_score.Coding & Debugging": 56.509433962264154,
    "WB_score": 54.53125,
    "WB_score.task_macro": 54.69508456618439,
    "Length": 2911.845703125,
    "Rank_ScoreMacro": 7,
    "Rank_TaskMacroReward.K": 37,
    "Rank_Avg": 22.0,
    "RewardScore_Avg": 27.347542283092196,
    "WB_Elo": 1242.030689140065
  },
  "deepseek-coder-v2": {
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "Arena Elo (hard-en) - latest": "-",
    "Arena Elo (hard-en) - 2024-07-16": "-",
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 54.49350649350649,
    "WB_score.Planning & Reasoning": 49.24698795180723,
    "WB_score.Math & Data Analysis": 41.59362549800797,
    "WB_score.Information/Advice seeking": 51.54228855721392,
    "WB_score.Coding & Debugging": 44.85714285714286,
    "WB_score": 48.895405669599214,
    "WB_score.task_macro": 47.39521235239142,
    "Length": 2795.3091265947005,
    "Rank_ScoreMacro": 20,
    "Rank_TaskMacroReward.K": 38,
    "Rank_Avg": 29.0,
    "RewardScore_Avg": 23.69760617619571,
    "WB_Elo": 1182.5200801513827
  },
  "gemma-2-9b-it": {
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "Arena Elo (hard-en) - 2024-07-16": 1155,
    "Arena Elo (hard-en) - latest": 1155,
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 51.007751937984494,
    "WB_score.Planning & Reasoning": 46.65667166416792,
    "WB_score.Math & Data Analysis": 36.42857142857142,
    "WB_score.Information/Advice seeking": 48.960396039603964,
    "WB_score.Coding & Debugging": 36.66666666666666,
    "WB_score": 45.36203522504893,
    "WB_score.task_macro": 42.696193124381026,
    "Length": 2802.8923679060667,
    "Rank_ScoreMacro": 27,
    "Rank_TaskMacroReward.K": 39,
    "Rank_Avg": 33.0,
    "RewardScore_Avg": 21.348096562190513,
    "WB_Elo": 1160.894855562556
  },
  "deepseek-v2-chat-0628": {
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "Arena Elo (hard-en) - 2024-07-16": 1233,
    "Arena Elo (hard-en) - latest": 1233,
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 56.43410852713178,
    "WB_score.Planning & Reasoning": 54.82810164424514,
    "WB_score.Math & Data Analysis": 51.42857142857142,
    "WB_score.Information/Advice seeking": 52.72277227722773,
    "WB_score.Coding & Debugging": 55.0,
    "WB_score": 53.80859375,
    "WB_score.task_macro": 53.994280411655694,
    "Length": 3252.376953125,
    "Rank_ScoreMacro": 8,
    "Rank_TaskMacroReward.K": 40,
    "Rank_Avg": 24.0,
    "RewardScore_Avg": 26.997140205827847,
    "WB_Elo": 1215.996561739598
  },
  "deepseek-v2-coder-0628": {
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "Arena Elo (hard-en) - 2024-07-16": 1204,
    "Arena Elo (hard-en) - latest": 1204,
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 40.775193798449614,
    "WB_score.Planning & Reasoning": 47.17488789237669,
    "WB_score.Math & Data Analysis": 46.42857142857142,
    "WB_score.Information/Advice seeking": 40.04950495049505,
    "WB_score.Coding & Debugging": 48.86792452830189,
    "WB_score": 43.4375,
    "WB_score.task_macro": 45.66459211926647,
    "Length": 2580.181640625,
    "Rank_ScoreMacro": 22,
    "Rank_TaskMacroReward.K": 41,
    "Rank_Avg": 31.5,
    "RewardScore_Avg": 22.832296059633236,
    "WB_Elo": 1195.7378839145724
  },
  "Athene-70B": {
    "Arena Elo (hard-en) - latest": "-",
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 60.36175710594314,
    "WB_score.Planning & Reasoning": 60.95952023988005,
    "WB_score.Math & Data Analysis": 57.13147410358566,
    "WB_score.Information/Advice seeking": 60.79207920792079,
    "WB_score.Coding & Debugging": 58.95734597156398,
    "WB_score": 59.41291585127202,
    "WB_score.task_macro": 59.53736733195851,
    "Length": 3175.1438356164385,
    "Rank_ScoreMacro": 1,
    "Rank_TaskMacroReward.K": 42,
    "Rank_Avg": 21.5,
    "RewardScore_Avg": 29.768683665979253,
    "WB_Elo": 1203.8347066684537
  },
  "gpt-4o-mini-2024-07-18": {
    "Arena Elo (hard-en) - latest": "-",
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 60.051679586563296,
    "WB_score.Planning & Reasoning": 58.23617339312406,
    "WB_score.Math & Data Analysis": 54.04761904761905,
    "WB_score.Information/Advice seeking": 57.42574257425743,
    "WB_score.Coding & Debugging": 57.16981132075471,
    "WB_score": 57.265625,
    "WB_score.task_macro": 57.13689403451416,
    "Length": 3648.126953125,
    "Rank_ScoreMacro": 3,
    "Rank_TaskMacroReward.K": 43,
    "Rank_Avg": 23.0,
    "RewardScore_Avg": 28.56844701725708,
    "WB_Elo": 1187.386766692505
  },
  "Mistral-Large-2": {
    "Arena Elo (hard-en) - latest": "-",
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 58.860103626943,
    "WB_score.Planning & Reasoning": 57.21556886227545,
    "WB_score.Math & Data Analysis": 52.66932270916335,
    "WB_score.Information/Advice seeking": 57.37623762376238,
    "WB_score.Coding & Debugging": 53.83886255924171,
    "WB_score": 55.80078125,
    "WB_score.task_macro": 55.56833516154802,
    "Length": 3503.6262230919765,
    "Rank_ScoreMacro": 4,
    "Rank_TaskMacroReward.K": 44,
    "Rank_Avg": 24.0,
    "RewardScore_Avg": 27.78416758077401,
    "WB_Elo": 1194.417583165205
  },
  "gemma-2-9b-it-DPO": {
    "Arena Elo (hard-en) - latest": "-",
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 59.067357512953365,
    "WB_score.Planning & Reasoning": 55.47226386806596,
    "WB_score.Math & Data Analysis": 47.12,
    "WB_score.Information/Advice seeking": 58.21782178217822,
    "WB_score.Coding & Debugging": 50.52132701421801,
    "WB_score": 54.2578125,
    "WB_score.task_macro": 53.22295446230848,
    "Length": 3982.628795298727,
    "Rank_ScoreMacro": 10,
    "Rank_TaskMacroReward.K": 45,
    "Rank_Avg": 27.5,
    "RewardScore_Avg": 26.61147723115424,
    "WB_Elo": 1175.9509120836465
  },
  "gemma-2-9b-it-SimPO": {
    "Arena Elo (hard-en) - latest": "-",
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 57.97927461139896,
    "WB_score.Planning & Reasoning": 55.645645645645644,
    "WB_score.Math & Data Analysis": 48.59437751004016,
    "WB_score.Information/Advice seeking": 56.485148514851474,
    "WB_score.Coding & Debugging": 50.857142857142854,
    "WB_score": 54.07624633431085,
    "WB_score.task_macro": 53.27923406955029,
    "Length": 4277.667647058824,
    "Rank_ScoreMacro": 9,
    "Rank_TaskMacroReward.K": 46,
    "Rank_Avg": 27.5,
    "RewardScore_Avg": 26.639617034775146,
    "WB_Elo": 1176.112557885973
  },
  "deepseekv2-chat": {
    "Arena Elo (hard-en) - latest": "-",
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 53.59173126614987,
    "WB_score.Planning & Reasoning": 50.62874251497006,
    "WB_score.Math & Data Analysis": 44.523809523809526,
    "WB_score.Information/Advice seeking": 51.811414392059554,
    "WB_score.Coding & Debugging": 44.43396226415095,
    "WB_score": 50.04887585532748,
    "WB_score.task_macro": 48.21191935259587,
    "Length": 2896.965786901271,
    "Rank_ScoreMacro": 17,
    "Rank_TaskMacroReward.K": 47,
    "Rank_Avg": 32.0,
    "RewardScore_Avg": 24.105959676297935,
    "WB_Elo": 1184.0030193945738
  },
  "gemma-2-27b-it@together": {
    "Arena Elo (hard-en) - latest": "-",
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 53.626943005181346,
    "WB_score.Planning & Reasoning": 50.55472263868065,
    "WB_score.Math & Data Analysis": 43.919999999999995,
    "WB_score.Information/Advice seeking": 50.49504950495049,
    "WB_score.Coding & Debugging": 47.01421800947868,
    "WB_score": 49.39453125,
    "WB_score.task_macro": 48.54019672452688,
    "Length": 2924.5455435847207,
    "Rank_ScoreMacro": 16,
    "Rank_TaskMacroReward.K": 48,
    "Rank_Avg": 32.0,
    "RewardScore_Avg": 24.27009836226344,
    "WB_Elo": 1180.9243651007419
  },
  "Mistral-Nemo-Instruct-2407": {
    "Arena Elo (hard-en) - latest": "-",
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 54.573643410852725,
    "WB_score.Planning & Reasoning": 47.41405082212257,
    "WB_score.Math & Data Analysis": 35.63492063492063,
    "WB_score.Information/Advice seeking": 51.93069306930694,
    "WB_score.Coding & Debugging": 39.71563981042655,
    "WB_score": 46.86217008797654,
    "WB_score.task_macro": 44.37513167010813,
    "Length": 3318.2130987292276,
    "Rank_ScoreMacro": 26,
    "Rank_TaskMacroReward.K": 49,
    "Rank_Avg": 37.5,
    "RewardScore_Avg": 22.187565835054066,
    "WB_Elo": 1164.8050583909894
  },
  "Llama-3-8B-Magpie-Align-v0.1": {
    "Arena Elo (hard-en) - latest": "-",
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 49.19896640826874,
    "WB_score.Planning & Reasoning": 42.7245508982036,
    "WB_score.Math & Data Analysis": 29.76000000000001,
    "WB_score.Information/Advice seeking": 48.910891089108915,
    "WB_score.Coding & Debugging": 33.74407582938389,
    "WB_score": 42.44618395303327,
    "WB_score.task_macro": 39.290196827463255,
    "Length": 3107.77397260274,
    "Rank_ScoreMacro": 29,
    "Rank_TaskMacroReward.K": 50,
    "Rank_Avg": 39.5,
    "RewardScore_Avg": 19.645098413731628,
    "WB_Elo": 1147.8553543925495
  },
  "Llama-3-Instruct-8B-SimPO-v0.2": {
    "Arena Elo (hard-en) - latest": "-",
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 51.83462532299741,
    "WB_score.Planning & Reasoning": 40.71856287425149,
    "WB_score.Math & Data Analysis": 24.38247011952191,
    "WB_score.Information/Advice seeking": 47.871287128712865,
    "WB_score.Coding & Debugging": 31.50943396226415,
    "WB_score": 41.50537634408602,
    "WB_score.task_macro": 37.1554198259368,
    "Length": 2533.764418377322,
    "Rank_ScoreMacro": 34,
    "Rank_TaskMacroReward.K": 51,
    "Rank_Avg": 42.5,
    "RewardScore_Avg": 18.5777099129684,
    "WB_Elo": 1149.8475673520386
  },
  "glm-4-9b-chat": {
    "Arena Elo (hard-en) - latest": "-",
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 47.751937984496124,
    "WB_score.Planning & Reasoning": 42.48502994011975,
    "WB_score.Math & Data Analysis": 29.800796812748995,
    "WB_score.Information/Advice seeking": 46.28712871287128,
    "WB_score.Coding & Debugging": 35.37735849056604,
    "WB_score": 41.17302052785924,
    "WB_score.task_macro": 39.09896797431742,
    "Length": 3692.043010752688,
    "Rank_ScoreMacro": 30,
    "Rank_TaskMacroReward.K": 52,
    "Rank_Avg": 41.0,
    "RewardScore_Avg": 19.54948398715871,
    "WB_Elo": 1144.1769664376288
  },
  "SELM-Llama-3-8B-Instruct-iter-3": {
    "Arena Elo (hard-en) - latest": "-",
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 51.05943152454781,
    "WB_score.Planning & Reasoning": 39.78978978978979,
    "WB_score.Math & Data Analysis": 23.505976095617527,
    "WB_score.Information/Advice seeking": 46.05459057071961,
    "WB_score.Coding & Debugging": 27.333333333333325,
    "WB_score": 39.96078431372549,
    "WB_score.task_macro": 35.25906077680738,
    "Length": 2913.1470588235293,
    "Rank_ScoreMacro": 37,
    "Rank_TaskMacroReward.K": 53,
    "Rank_Avg": 45.0,
    "RewardScore_Avg": 17.62953038840369,
    "WB_Elo": 1143.220584018321
  },
  "Yi-1.5-9B-Chat": {
    "Arena Elo (hard-en) - latest": "-",
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 45.5958549222798,
    "WB_score.Planning & Reasoning": 42.37237237237237,
    "WB_score.Math & Data Analysis": 32.20883534136546,
    "WB_score.Information/Advice seeking": 42.62376237623762,
    "WB_score.Coding & Debugging": 34.97630331753555,
    "WB_score": 39.8435972629521,
    "WB_score.task_macro": 38.66535351517231,
    "Length": 3468.23431372549,
    "Rank_ScoreMacro": 33,
    "Rank_TaskMacroReward.K": 54,
    "Rank_Avg": 43.5,
    "RewardScore_Avg": 19.332676757586157,
    "WB_Elo": 1138.9084499336473
  },
  "Llama-3-Instruct-8B-SimPO-ExPO": {
    "Arena Elo (hard-en) - latest": "-",
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 49.14728682170542,
    "WB_score.Planning & Reasoning": 39.46107784431138,
    "WB_score.Math & Data Analysis": 21.195219123505975,
    "WB_score.Information/Advice seeking": 47.32673267326733,
    "WB_score.Coding & Debugging": 28.584905660377355,
    "WB_score": 39.687194525904204,
    "WB_score.task_macro": 35.01502977266739,
    "Length": 2480.6490713587486,
    "Rank_ScoreMacro": 38,
    "Rank_TaskMacroReward.K": 55,
    "Rank_Avg": 46.5,
    "RewardScore_Avg": 17.507514886333695,
    "WB_Elo": 1146.7960840607486
  },
  "Starling-LM-7B-beta-ExPO": {
    "Arena Elo (hard-en) - latest": "-",
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 44.30051813471502,
    "WB_score.Planning & Reasoning": 36.31736526946108,
    "WB_score.Math & Data Analysis": 18.571428571428577,
    "WB_score.Information/Advice seeking": 42.871287128712865,
    "WB_score.Coding & Debugging": 25.308056872037916,
    "WB_score": 35.01466275659824,
    "WB_score.task_macro": 31.559353823619887,
    "Length": 2835.826810176125,
    "Rank_ScoreMacro": 40,
    "Rank_TaskMacroReward.K": 56,
    "Rank_Avg": 48.0,
    "RewardScore_Avg": 15.779676911809943,
    "WB_Elo": 1128.4219466112934
  },
  "gemma-2-2b-it": {
    "Arena Elo (hard-en) - latest": "-",
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 43.61757105943152,
    "WB_score.Planning & Reasoning": 33.811659192825104,
    "WB_score.Math & Data Analysis": 15.79365079365079,
    "WB_score.Information/Advice seeking": 39.90099009900991,
    "WB_score.Coding & Debugging": 17.904761904761912,
    "WB_score": 32.72015655577299,
    "WB_score.task_macro": 27.826043214654263,
    "Length": 3589.3894324853227,
    "Rank_ScoreMacro": 50,
    "Rank_TaskMacroReward.K": 57,
    "Rank_Avg": 53.5,
    "RewardScore_Avg": 13.913021607327131,
    "WB_Elo": 1112.519846831341
  },
  "neo_7b_instruct_v0.1": {
    "Arena Elo (hard-en) - latest": "-",
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 39.48186528497409,
    "WB_score.Planning & Reasoning": 31.44992526158445,
    "WB_score.Math & Data Analysis": 15.0,
    "WB_score.Information/Advice seeking": 36.33663366336634,
    "WB_score.Coding & Debugging": 14.02843601895734,
    "WB_score": 29.19921875,
    "WB_score.task_macro": 25.019233576987165,
    "Length": 3735.800586510264,
    "Rank_ScoreMacro": 54,
    "Rank_TaskMacroReward.K": 58,
    "Rank_Avg": 56.0,
    "RewardScore_Avg": 12.509616788493583,
    "WB_Elo": 1101.6547136465722
  },
  "neo_7b_instruct_v0.1-ExPO": {
    "Arena Elo (hard-en) - latest": "-",
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 38.549222797927456,
    "WB_score.Planning & Reasoning": 28.669656203288483,
    "WB_score.Math & Data Analysis": 12.589641434262955,
    "WB_score.Information/Advice seeking": 34.85148514851485,
    "WB_score.Coding & Debugging": 12.76190476190477,
    "WB_score": 27.624633431085037,
    "WB_score.task_macro": 23.114172189706185,
    "Length": 4107.917808219178,
    "Rank_ScoreMacro": 57,
    "Rank_TaskMacroReward.K": 59,
    "Rank_Avg": 58.0,
    "RewardScore_Avg": 11.557086094853092,
    "WB_Elo": 1101.3140217902342
  },
  "ul1_15": {
    "Arena Elo (hard-en) - latest": "-",
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 20.0,
    "WB_score.Planning & Reasoning": 40.0,
    "WB_score.Math & Data Analysis": 40.0,
    "WB_score.Information/Advice seeking": 0,
    "WB_score.Coding & Debugging": 20.0,
    "WB_score": 26.66666666666666,
    "WB_score.task_macro": 26.315789473684212,
    "Length": 5430.666666666667,
    "Rank_ScoreMacro": 51,
    "Rank_TaskMacroReward.K": 60,
    "Rank_Avg": 55.5,
    "RewardScore_Avg": 13.157894736842106,
    "WB_Elo": 1075.2221441024715
  },
  "Yi-1.5-6B-Chat": {
    "Arena Elo (hard-en) - latest": "-",
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 31.088082901554408,
    "WB_score.Planning & Reasoning": 27.2972972972973,
    "WB_score.Math & Data Analysis": 16.799999999999997,
    "WB_score.Information/Advice seeking": 31.414392059553347,
    "WB_score.Coding & Debugging": 16.587677725118475,
    "WB_score": 25.278592375366564,
    "WB_score.task_macro": 23.31811668914988,
    "Length": 3899.4686274509804,
    "Rank_ScoreMacro": 56,
    "Rank_TaskMacroReward.K": 61,
    "Rank_Avg": 58.5,
    "RewardScore_Avg": 11.65905834457494,
    "WB_Elo": 1099.9077555854667
  },
  "reka-edge": {
    "Arena Elo (hard-en) - latest": "-",
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 36.180371352785144,
    "WB_score.Planning & Reasoning": 25.007727975270484,
    "WB_score.Math & Data Analysis": 8.89795918367346,
    "WB_score.Information/Advice seeking": 34.3896103896104,
    "WB_score.Coding & Debugging": 13.526570048309186,
    "WB_score": 23.186705767350926,
    "WB_score.task_macro": 21.25225793299967,
    "Length": 2417.351106639839,
    "Rank_ScoreMacro": 58,
    "Rank_TaskMacroReward.K": 62,
    "Rank_Avg": 60.0,
    "RewardScore_Avg": 10.626128966499834,
    "WB_Elo": 1099.955143765188
  },
  "adv_1_165": {
    "Arena Elo (hard-en) - latest": "-",
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 21.95652173913043,
    "WB_score.Planning & Reasoning": 12.542372881355934,
    "WB_score.Math & Data Analysis": -12.3076923076923,
    "WB_score.Information/Advice seeking": 17.346938775510203,
    "WB_score.Coding & Debugging": 0.39215686274509665,
    "WB_score": 11.5625,
    "WB_score.task_macro": 5.862933647080391,
    "Length": 2971.6156862745097,
    "Rank_ScoreMacro": 62,
    "Rank_TaskMacroReward.K": 63,
    "Rank_Avg": 62.5,
    "RewardScore_Avg": 2.9314668235401955,
    "WB_Elo": 1074.0436367857062
  },
  "adv_3_465": {
    "Arena Elo (hard-en) - latest": "-",
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 21.0752688172043,
    "WB_score.Planning & Reasoning": 10.282485875706211,
    "WB_score.Math & Data Analysis": -8.307692307692314,
    "WB_score.Information/Advice seeking": 14.285714285714288,
    "WB_score.Coding & Debugging": -6.15384615384615,
    "WB_score": 9.375,
    "WB_score.task_macro": 3.811584730004343,
    "Length": 1877.9140625,
    "Rank_ScoreMacro": 64,
    "Rank_TaskMacroReward.K": 64,
    "Rank_Avg": 64.0,
    "RewardScore_Avg": 1.9057923650021715,
    "WB_Elo": 1077.3718706341317
  },
  "adv_3_90": {
    "Arena Elo (hard-en) - latest": "-",
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 14.6236559139785,
    "WB_score.Planning & Reasoning": 11.412429378531073,
    "WB_score.Math & Data Analysis": -9.846153846153847,
    "WB_score.Information/Advice seeking": 16.530612244897966,
    "WB_score.Coding & Debugging": -2.6923076923077005,
    "WB_score": 9.296875,
    "WB_score.task_macro": 4.371323242586967,
    "Length": 2411.0625,
    "Rank_ScoreMacro": 63,
    "Rank_TaskMacroReward.K": 65,
    "Rank_Avg": 64.0,
    "RewardScore_Avg": 2.1856616212934834,
    "WB_Elo": 1069.7585798882146
  },
  "adv_3_165": {
    "Arena Elo (hard-en) - latest": "-",
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 15.91397849462366,
    "WB_score.Planning & Reasoning": 9.604519774011298,
    "WB_score.Math & Data Analysis": -10.9375,
    "WB_score.Information/Advice seeking": 13.87755102040817,
    "WB_score.Coding & Debugging": -4.399999999999995,
    "WB_score": 8.18897637795276,
    "WB_score.task_macro": 2.9333267852909657,
    "Length": 2528.8740157480315,
    "Rank_ScoreMacro": 67,
    "Rank_TaskMacroReward.K": 66,
    "Rank_Avg": 66.5,
    "RewardScore_Avg": 1.4666633926454828,
    "WB_Elo": 1064.2063539127864
  },
  "adv_2_465": {
    "Arena Elo (hard-en) - latest": "-",
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 19.130434782608692,
    "WB_score.Planning & Reasoning": 9.717514124293789,
    "WB_score.Math & Data Analysis": -9.846153846153847,
    "WB_score.Information/Advice seeking": 12.448979591836729,
    "WB_score.Coding & Debugging": -4.313725490196081,
    "WB_score": 7.578125,
    "WB_score.task_macro": 3.3285334803474065,
    "Length": 1942.6039215686274,
    "Rank_ScoreMacro": 66,
    "Rank_TaskMacroReward.K": 67,
    "Rank_Avg": 66.5,
    "RewardScore_Avg": 1.6642667401737032,
    "WB_Elo": 1068.5902659798521
  },
  "adv_3_390": {
    "Arena Elo (hard-en) - latest": "-",
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 17.63440860215054,
    "WB_score.Planning & Reasoning": 9.491525423728806,
    "WB_score.Math & Data Analysis": -8.307692307692314,
    "WB_score.Information/Advice seeking": 14.897959183673475,
    "WB_score.Coding & Debugging": -5.384615384615383,
    "WB_score": 7.265625,
    "WB_score.task_macro": 3.5403408273747035,
    "Length": 2100.68359375,
    "Rank_ScoreMacro": 65,
    "Rank_TaskMacroReward.K": 68,
    "Rank_Avg": 66.5,
    "RewardScore_Avg": 1.7701704136873517,
    "WB_Elo": 1064.8520966374954
  },
  "adv_2_315": {
    "Arena Elo (hard-en) - latest": "-",
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 19.565217391304355,
    "WB_score.Planning & Reasoning": 6.553672316384187,
    "WB_score.Math & Data Analysis": -11.692307692307686,
    "WB_score.Information/Advice seeking": 11.020408163265305,
    "WB_score.Coding & Debugging": -6.274509803921564,
    "WB_score": 7.03125,
    "WB_score.task_macro": 1.4114863718677364,
    "Length": 1891.6392156862746,
    "Rank_ScoreMacro": 71,
    "Rank_TaskMacroReward.K": 69,
    "Rank_Avg": 70.0,
    "RewardScore_Avg": 0.7057431859338682,
    "WB_Elo": 1067.452013307171
  },
  "adv_1_315": {
    "Arena Elo (hard-en) - latest": "-",
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 19.347826086956523,
    "WB_score.Planning & Reasoning": 9.717514124293789,
    "WB_score.Math & Data Analysis": -8.61538461538462,
    "WB_score.Information/Advice seeking": 11.632653061224492,
    "WB_score.Coding & Debugging": -6.666666666666661,
    "WB_score": 6.953125,
    "WB_score.task_macro": 2.8624373781149304,
    "Length": 1979.6313725490197,
    "Rank_ScoreMacro": 68,
    "Rank_TaskMacroReward.K": 70,
    "Rank_Avg": 69.0,
    "RewardScore_Avg": 1.4312186890574652,
    "WB_Elo": 1068.6828356171952
  },
  "adv_3_315": {
    "Arena Elo (hard-en) - latest": "-",
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 18.06451612903226,
    "WB_score.Planning & Reasoning": 6.4406779661016955,
    "WB_score.Math & Data Analysis": -9.846153846153847,
    "WB_score.Information/Advice seeking": 12.04081632653061,
    "WB_score.Coding & Debugging": -4.2307692307692335,
    "WB_score": 6.875,
    "WB_score.task_macro": 2.3113899752475406,
    "Length": 1984.375,
    "Rank_ScoreMacro": 70,
    "Rank_TaskMacroReward.K": 71,
    "Rank_Avg": 70.5,
    "RewardScore_Avg": 1.1556949876237703,
    "WB_Elo": 1064.4179912206985
  },
  "adv_1_615": {
    "Arena Elo (hard-en) - latest": "-",
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 19.565217391304355,
    "WB_score.Planning & Reasoning": 8.700564971751419,
    "WB_score.Math & Data Analysis": -11.076923076923073,
    "WB_score.Information/Advice seeking": 10.816326530612237,
    "WB_score.Coding & Debugging": -11.764705882352935,
    "WB_score": 6.40625,
    "WB_score.task_macro": 0.6290009217760815,
    "Length": 1929.9607843137255,
    "Rank_ScoreMacro": 76,
    "Rank_TaskMacroReward.K": 72,
    "Rank_Avg": 74.0,
    "RewardScore_Avg": 0.31450046088804073,
    "WB_Elo": 1064.042212421767
  },
  "adv_2_615": {
    "Arena Elo (hard-en) - latest": "-",
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 17.391304347826093,
    "WB_score.Planning & Reasoning": 8.24858757062147,
    "WB_score.Math & Data Analysis": -8.923076923076927,
    "WB_score.Information/Advice seeking": 10.0,
    "WB_score.Coding & Debugging": -3.9215686274509842,
    "WB_score": 6.40625,
    "WB_score.task_macro": 2.6697576694314162,
    "Length": 1757.0392156862745,
    "Rank_ScoreMacro": 69,
    "Rank_TaskMacroReward.K": 73,
    "Rank_Avg": 71.0,
    "RewardScore_Avg": 1.3348788347157081,
    "WB_Elo": 1065.591255298217
  },
  "adv_2_240": {
    "Arena Elo (hard-en) - latest": "-",
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 18.04347826086957,
    "WB_score.Planning & Reasoning": 6.779661016949152,
    "WB_score.Math & Data Analysis": -12.3076923076923,
    "WB_score.Information/Advice seeking": 11.632653061224492,
    "WB_score.Coding & Debugging": -5.8823529411764675,
    "WB_score": 6.328125,
    "WB_score.task_macro": 1.3810887817635178,
    "Length": 1892.137254901961,
    "Rank_ScoreMacro": 72,
    "Rank_TaskMacroReward.K": 74,
    "Rank_Avg": 73.0,
    "RewardScore_Avg": 0.6905443908817589,
    "WB_Elo": 1062.4644905323082
  },
  "adv_1_540": {
    "Arena Elo (hard-en) - latest": "-",
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 18.695652173913047,
    "WB_score.Planning & Reasoning": 8.926553672316384,
    "WB_score.Math & Data Analysis": -11.076923076923073,
    "WB_score.Information/Advice seeking": 9.591836734693882,
    "WB_score.Coding & Debugging": -11.372549019607838,
    "WB_score": 6.25,
    "WB_score.task_macro": 0.5067971319872724,
    "Length": 1812.2470588235294,
    "Rank_ScoreMacro": 77,
    "Rank_TaskMacroReward.K": 75,
    "Rank_Avg": 76.0,
    "RewardScore_Avg": 0.2533985659936362,
    "WB_Elo": 1067.8970295075676
  },
  "adv_2_15": {
    "Arena Elo (hard-en) - latest": "-",
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 15.26881720430108,
    "WB_score.Planning & Reasoning": 6.21468926553673,
    "WB_score.Math & Data Analysis": -12.92307692307693,
    "WB_score.Information/Advice seeking": 12.857142857142865,
    "WB_score.Coding & Debugging": -9.615384615384617,
    "WB_score": 5.546875,
    "WB_score.task_macro": 0.021751502025452583,
    "Length": 4086.94140625,
    "Rank_ScoreMacro": 79,
    "Rank_TaskMacroReward.K": 76,
    "Rank_Avg": 77.5,
    "RewardScore_Avg": 0.010875751012726292,
    "WB_Elo": 1079.908039416025
  },
  "adv_3_615": {
    "Arena Elo (hard-en) - latest": "-",
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 16.7741935483871,
    "WB_score.Planning & Reasoning": 7.570621468926557,
    "WB_score.Math & Data Analysis": -7.0769230769230695,
    "WB_score.Information/Advice seeking": 8.16326530612244,
    "WB_score.Coding & Debugging": -9.230769230769234,
    "WB_score": 5.46875,
    "WB_score.task_macro": 1.127881678854519,
    "Length": 2028.69140625,
    "Rank_ScoreMacro": 73,
    "Rank_TaskMacroReward.K": 77,
    "Rank_Avg": 75.0,
    "RewardScore_Avg": 0.5639408394272595,
    "WB_Elo": 1066.6179641454535
  },
  "adv_3_690": {
    "Arena Elo (hard-en) - latest": "-",
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 17.20430107526882,
    "WB_score.Planning & Reasoning": 8.474576271186436,
    "WB_score.Math & Data Analysis": -7.6923076923077005,
    "WB_score.Information/Advice seeking": 10.0,
    "WB_score.Coding & Debugging": -12.6923076923077,
    "WB_score": 5.46875,
    "WB_score.task_macro": 0.6605639092473954,
    "Length": 1880.02734375,
    "Rank_ScoreMacro": 75,
    "Rank_TaskMacroReward.K": 78,
    "Rank_Avg": 76.5,
    "RewardScore_Avg": 0.3302819546236977,
    "WB_Elo": 1063.43159238328
  },
  "adv_1_465": {
    "Arena Elo (hard-en) - latest": "-",
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 18.913043478260878,
    "WB_score.Planning & Reasoning": 7.344632768361574,
    "WB_score.Math & Data Analysis": -14.153846153846157,
    "WB_score.Information/Advice seeking": 11.224489795918373,
    "WB_score.Coding & Debugging": -11.372549019607838,
    "WB_score": 5.390625,
    "WB_score.task_macro": -0.2766005014388985,
    "Length": 1798.521568627451,
    "Rank_ScoreMacro": 81,
    "Rank_TaskMacroReward.K": 79,
    "Rank_Avg": 80.0,
    "RewardScore_Avg": -0.13830025071944926,
    "WB_Elo": 1062.2241543146024
  },
  "adv_1_390": {
    "Arena Elo (hard-en) - latest": "-",
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 19.130434782608692,
    "WB_score.Planning & Reasoning": 6.21468926553673,
    "WB_score.Math & Data Analysis": -10.46153846153846,
    "WB_score.Information/Advice seeking": 7.5510204081632715,
    "WB_score.Coding & Debugging": -6.666666666666661,
    "WB_score": 5.3125,
    "WB_score.task_macro": 0.8846257862054583,
    "Length": 1990.8235294117646,
    "Rank_ScoreMacro": 74,
    "Rank_TaskMacroReward.K": 80,
    "Rank_Avg": 77.0,
    "RewardScore_Avg": 0.44231289310272914,
    "WB_Elo": 1066.5121727491298
  },
  "adv_3_760": {
    "Arena Elo (hard-en) - latest": "-",
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 15.91397849462366,
    "WB_score.Planning & Reasoning": 7.457627118644066,
    "WB_score.Math & Data Analysis": -7.384615384615376,
    "WB_score.Information/Advice seeking": 8.571428571428577,
    "WB_score.Coding & Debugging": -13.84615384615385,
    "WB_score": 4.921875,
    "WB_score.task_macro": -0.20730802486723046,
    "Length": 1921.4609375,
    "Rank_ScoreMacro": 80,
    "Rank_TaskMacroReward.K": 81,
    "Rank_Avg": 80.5,
    "RewardScore_Avg": -0.10365401243361523,
    "WB_Elo": 1063.709145129158
  },
  "adv_3_240": {
    "Arena Elo (hard-en) - latest": "-",
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 14.19354838709678,
    "WB_score.Planning & Reasoning": 7.2316384180791005,
    "WB_score.Math & Data Analysis": -13.538461538461544,
    "WB_score.Information/Advice seeking": 11.428571428571423,
    "WB_score.Coding & Debugging": -8.461538461538467,
    "WB_score": 4.921875,
    "WB_score.task_macro": 0.12470866782972746,
    "Length": 1760.671875,
    "Rank_ScoreMacro": 78,
    "Rank_TaskMacroReward.K": 82,
    "Rank_Avg": 80.0,
    "RewardScore_Avg": 0.06235433391486373,
    "WB_Elo": 1061.3690088560227
  },
  "adv_3_15": {
    "Arena Elo (hard-en) - latest": "-",
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 15.26881720430108,
    "WB_score.Planning & Reasoning": 6.21468926553673,
    "WB_score.Math & Data Analysis": -11.38461538461538,
    "WB_score.Information/Advice seeking": 8.16326530612244,
    "WB_score.Coding & Debugging": -10.196078431372548,
    "WB_score": 4.7058823529411775,
    "WB_score.task_macro": -0.5483145810879542,
    "Length": 3994.5686274509803,
    "Rank_ScoreMacro": 84,
    "Rank_TaskMacroReward.K": 83,
    "Rank_Avg": 83.5,
    "RewardScore_Avg": -0.2741572905439771,
    "WB_Elo": 1073.5852697119906
  },
  "adv_2_390": {
    "Arena Elo (hard-en) - latest": "-",
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 15.48387096774194,
    "WB_score.Planning & Reasoning": 6.101694915254239,
    "WB_score.Math & Data Analysis": -11.692307692307686,
    "WB_score.Information/Advice seeking": 12.04081632653061,
    "WB_score.Coding & Debugging": -13.461538461538467,
    "WB_score": 4.6875,
    "WB_score.task_macro": -0.8672767150303251,
    "Length": 1846.99609375,
    "Rank_ScoreMacro": 85,
    "Rank_TaskMacroReward.K": 84,
    "Rank_Avg": 84.5,
    "RewardScore_Avg": -0.43363835751516255,
    "WB_Elo": 1063.2083841516412
  },
  "adv_3_540": {
    "Arena Elo (hard-en) - latest": "-",
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 13.76344086021506,
    "WB_score.Planning & Reasoning": 6.21468926553673,
    "WB_score.Math & Data Analysis": -10.46153846153846,
    "WB_score.Information/Advice seeking": 10.204081632653068,
    "WB_score.Coding & Debugging": -11.15384615384615,
    "WB_score": 4.140625,
    "WB_score.task_macro": -0.4422532457532429,
    "Length": 1948.578125,
    "Rank_ScoreMacro": 83,
    "Rank_TaskMacroReward.K": 85,
    "Rank_Avg": 84.0,
    "RewardScore_Avg": -0.22112662287662144,
    "WB_Elo": 1065.3092709026575
  },
  "adv_2_690": {
    "Arena Elo (hard-en) - latest": "-",
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 19.120879120879124,
    "WB_score.Planning & Reasoning": 4.886363636363633,
    "WB_score.Math & Data Analysis": -17.230769230769223,
    "WB_score.Information/Advice seeking": 6.530612244897966,
    "WB_score.Coding & Debugging": -9.411764705882355,
    "WB_score": 4.078431372549023,
    "WB_score.task_macro": -1.7745519628535975,
    "Length": 1851.9448818897638,
    "Rank_ScoreMacro": 89,
    "Rank_TaskMacroReward.K": 86,
    "Rank_Avg": 87.5,
    "RewardScore_Avg": -0.8872759814267988,
    "WB_Elo": 1056.7342849017814
  },
  "adv_2_165": {
    "Arena Elo (hard-en) - latest": "-",
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 20.21505376344086,
    "WB_score.Planning & Reasoning": 7.570621468926557,
    "WB_score.Math & Data Analysis": -15.076923076923077,
    "WB_score.Information/Advice seeking": 5.306122448979593,
    "WB_score.Coding & Debugging": -13.076923076923084,
    "WB_score": 3.984375,
    "WB_score.task_macro": -1.6574066038870758,
    "Length": 2195.203125,
    "Rank_ScoreMacro": 88,
    "Rank_TaskMacroReward.K": 87,
    "Rank_Avg": 87.5,
    "RewardScore_Avg": -0.8287033019435379,
    "WB_Elo": 1066.4341715209573
  },
  "adv_2_760": {
    "Arena Elo (hard-en) - latest": "-",
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 21.304347826086953,
    "WB_score.Planning & Reasoning": 5.310734463276834,
    "WB_score.Math & Data Analysis": -16.61538461538461,
    "WB_score.Information/Advice seeking": 5.102040816326525,
    "WB_score.Coding & Debugging": -13.725490196078436,
    "WB_score": 3.984375,
    "WB_score.task_macro": -2.664236790757524,
    "Length": 1735.3294117647058,
    "Rank_ScoreMacro": 94,
    "Rank_TaskMacroReward.K": 88,
    "Rank_Avg": 91.0,
    "RewardScore_Avg": -1.332118395378762,
    "WB_Elo": 1055.277658502437
  },
  "adv_1_15": {
    "Arena Elo (hard-en) - latest": "-",
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 11.612903225806459,
    "WB_score.Planning & Reasoning": 4.858757062146886,
    "WB_score.Math & Data Analysis": -9.230769230769234,
    "WB_score.Information/Advice seeking": 10.612244897959187,
    "WB_score.Coding & Debugging": -10.0,
    "WB_score": 3.828125,
    "WB_score.task_macro": -0.3982500245711593,
    "Length": 4155.234375,
    "Rank_ScoreMacro": 82,
    "Rank_TaskMacroReward.K": 89,
    "Rank_Avg": 85.5,
    "RewardScore_Avg": -0.19912501228557966,
    "WB_Elo": 1078.1495083108457
  },
  "adv_2_540": {
    "Arena Elo (hard-en) - latest": "-",
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 17.17391304347826,
    "WB_score.Planning & Reasoning": 5.423728813559325,
    "WB_score.Math & Data Analysis": -13.230769230769237,
    "WB_score.Information/Advice seeking": 8.775510204081627,
    "WB_score.Coding & Debugging": -14.50980392156863,
    "WB_score": 3.828125,
    "WB_score.task_macro": -1.9831103033643191,
    "Length": 1849.764705882353,
    "Rank_ScoreMacro": 91,
    "Rank_TaskMacroReward.K": 90,
    "Rank_Avg": 90.5,
    "RewardScore_Avg": -0.9915551516821596,
    "WB_Elo": 1060.1585574788016
  },
  "adv_1_90": {
    "Arena Elo (hard-en) - latest": "-",
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 12.68817204301076,
    "WB_score.Planning & Reasoning": 6.892655367231644,
    "WB_score.Math & Data Analysis": -11.692307692307686,
    "WB_score.Information/Advice seeking": 8.16326530612244,
    "WB_score.Coding & Debugging": -11.538461538461533,
    "WB_score": 3.515625,
    "WB_score.task_macro": -1.0595853484732292,
    "Length": 2511.82421875,
    "Rank_ScoreMacro": 86,
    "Rank_TaskMacroReward.K": 91,
    "Rank_Avg": 88.5,
    "RewardScore_Avg": -0.5297926742366146,
    "WB_Elo": 1056.860285412855
  },
  "adv_1_690": {
    "Arena Elo (hard-en) - latest": "-",
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 14.130434782608692,
    "WB_score.Planning & Reasoning": 5.310734463276834,
    "WB_score.Math & Data Analysis": -10.769230769230766,
    "WB_score.Information/Advice seeking": 9.591836734693882,
    "WB_score.Coding & Debugging": -14.000000000000004,
    "WB_score": 3.125,
    "WB_score.task_macro": -1.5519405784863096,
    "Length": 1741.9251968503936,
    "Rank_ScoreMacro": 87,
    "Rank_TaskMacroReward.K": 92,
    "Rank_Avg": 89.5,
    "RewardScore_Avg": -0.7759702892431548,
    "WB_Elo": 1061.9282229931644
  },
  "adv_1_760": {
    "Arena Elo (hard-en) - latest": "-",
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 15.434782608695645,
    "WB_score.Planning & Reasoning": 5.649717514124291,
    "WB_score.Math & Data Analysis": -11.38461538461538,
    "WB_score.Information/Advice seeking": 9.387755102040813,
    "WB_score.Coding & Debugging": -15.199999999999996,
    "WB_score": 2.890625,
    "WB_score.task_macro": -1.8030022865434903,
    "Length": 1772.0118110236222,
    "Rank_ScoreMacro": 90,
    "Rank_TaskMacroReward.K": 93,
    "Rank_Avg": 91.5,
    "RewardScore_Avg": -0.9015011432717451,
    "WB_Elo": 1060.872351458651
  },
  "adv_1_240": {
    "Arena Elo (hard-en) - latest": "-",
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 13.478260869565215,
    "WB_score.Planning & Reasoning": 6.666666666666661,
    "WB_score.Math & Data Analysis": -11.38461538461538,
    "WB_score.Information/Advice seeking": 6.938775510204085,
    "WB_score.Coding & Debugging": -14.901960784313726,
    "WB_score": 2.578125,
    "WB_score.task_macro": -2.049583360892324,
    "Length": 1858.556862745098,
    "Rank_ScoreMacro": 92,
    "Rank_TaskMacroReward.K": 94,
    "Rank_Avg": 93.0,
    "RewardScore_Avg": -1.024791680446162,
    "WB_Elo": 1060.5292606328428
  },
  "adv_2_90": {
    "Arena Elo (hard-en) - latest": "-",
    "Arena-Hard v0.1": "-",
    "AE2.0 LC": "-",
    "AE2.0": "-",
    "haiku_reward.K=2000": 0,
    "llama_reward.K=2000": 0,
    "gpt4t_reward.K=2000": 0,
    "haiku_reward.Creative Tasks.K=2000": 0,
    "llama_reward.Creative Tasks.K=2000": 0,
    "gpt4t_reward.Creative Tasks.K=2000": 0,
    "mixture_of_rewards.Creative Tasks.K=2000": 0.0,
    "haiku_reward.Planning & Reasoning.K=2000": 0,
    "llama_reward.Planning & Reasoning.K=2000": 0,
    "gpt4t_reward.Planning & Reasoning.K=2000": 0,
    "mixture_of_rewards.Planning & Reasoning.K=2000": 0.0,
    "haiku_reward.Math & Data Analysis.K=2000": 0,
    "llama_reward.Math & Data Analysis.K=2000": 0,
    "gpt4t_reward.Math & Data Analysis.K=2000": 0,
    "mixture_of_rewards.Math & Data Analysis.K=2000": 0.0,
    "haiku_reward.Information/Advice seeking.K=2000": 0,
    "llama_reward.Information/Advice seeking.K=2000": 0,
    "gpt4t_reward.Information/Advice seeking.K=2000": 0,
    "mixture_of_rewards.Information/Advice seeking.K=2000": 0.0,
    "haiku_reward.Coding & Debugging.K=2000": 0,
    "llama_reward.Coding & Debugging.K=2000": 0,
    "gpt4t_reward.Coding & Debugging.K=2000": 0,
    "mixture_of_rewards.Coding & Debugging.K=2000": 0.0,
    "haiku_reward.task_macro.K=2000": 0,
    "llama_reward.task_macro.K=2000": 0,
    "gpt4t_reward.task_macro.K=2000": 0,
    "mixture_of_rewards.K=2000": 0.0,
    "task_macro_reward.K=2000": 0.0,
    "WB_score.Creative Tasks": 12.68817204301076,
    "WB_score.Planning & Reasoning": 4.971751412429377,
    "WB_score.Math & Data Analysis": -13.538461538461544,
    "WB_score.Information/Advice seeking": 4.285714285714288,
    "WB_score.Coding & Debugging": -10.769230769230766,
    "WB_score": 2.265625,
    "WB_score.task_macro": -2.363566104983618,
    "Length": 2588.91796875,
    "Rank_ScoreMacro": 93,
    "Rank_TaskMacroReward.K": 95,
    "Rank_Avg": 94.0,
    "RewardScore_Avg": -1.181783052491809,
    "WB_Elo": 1060.2008173894271
  }
}